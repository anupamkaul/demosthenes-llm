python test_MultiHeadAttnV1.py     
torch.Size([2, 6, 3])
tensor([[[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]],

        [[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]]])

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],
         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],
         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],
         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],

        [[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],
         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],
         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],
         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],
         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],
         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],

        [[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],
         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],
         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],
         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],
         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],
         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],
         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],
         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],
         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6041, 0.6459, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3988, 0.4260, 0.4252, 0.0000, 0.0000, 0.0000],
         [0.3056, 0.3182, 0.3178, 0.3085, 0.0000, 0.0000],
         [0.2493, 0.2575, 0.2573, 0.2418, 0.2441, 0.0000],
         [0.2031, 0.2136, 0.2133, 0.2068, 0.2031, 0.2102]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6041, 0.6459, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3056, 0.3182, 0.3178, 0.3085, 0.0000, 0.0000],
         [0.2493, 0.2575, 0.2573, 0.2418, 0.2441, 0.0000],
         [0.2031, 0.2136, 0.2133, 0.0000, 0.0000, 0.2102]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.2327,  0.1055,  0.1098,  0.0913,  0.1549,  0.0521],
         [-0.2396,  0.1015,  0.1057,  0.0902,  0.1501,  0.0518],
         [-0.2323,  0.1004,  0.1045,  0.0885,  0.1481,  0.0507],
         [-0.1344,  0.0502,  0.0523,  0.0470,  0.0753,  0.0272],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0174],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],

        [[-0.2327,  0.1055,  0.1098,  0.0913,  0.1549,  0.0521],
         [-0.2396,  0.1015,  0.1057,  0.0902,  0.1501,  0.0518],
         [-0.2323,  0.1004,  0.1045,  0.0885,  0.1481,  0.0507],
         [-0.1344,  0.0502,  0.0523,  0.0470,  0.0753,  0.0272],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0174],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],
         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],
         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],

        [[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],
         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],
         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],
         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],
         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],
         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],
         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],
         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],
         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5500, 0.7000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3537, 0.4475, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3224, 0.3229, 0.3217, 0.0000, 0.0000],
         [0.0000, 0.2529, 0.2533, 0.2496, 0.2563, 0.0000],
         [0.1760, 0.2144, 0.2148, 0.2146, 0.2198, 0.2105]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3537, 0.4475, 0.4488, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3224, 0.3229, 0.3217, 0.0000, 0.0000],
         [0.2379, 0.2529, 0.2533, 0.2496, 0.2563, 0.0000],
         [0.1760, 0.0000, 0.0000, 0.2146, 0.2198, 0.2105]]],
       grad_fn=<MulBackward0>)
context vecs: 
 tensor([[[-0.5649,  0.2770,  0.5965,  0.1329],
         [-0.7343,  0.0072,  0.7363,  0.4071],
         [-0.7875, -0.0790,  0.4717,  0.2605],
         [-0.7093, -0.1053,  0.6847,  0.4487],
         [-0.6907, -0.1226,  0.5516,  0.4032],
         [-0.6624, -0.1351,  0.6346,  0.4367]],

        [[-0.5649,  0.2770,  0.5965,  0.1329],
         [-0.7343,  0.0072,  0.2624,  0.0585],
         [ 0.0000,  0.0000,  0.7752,  0.4825],
         [-0.7093, -0.1053,  0.6847,  0.4487],
         [-0.6907, -0.1226,  0.6651,  0.4285],
         [-0.4849, -0.0705,  0.3442,  0.2237]]], grad_fn=<CatBackward0>)
context vecs shape:  torch.Size([2, 6, 4])

