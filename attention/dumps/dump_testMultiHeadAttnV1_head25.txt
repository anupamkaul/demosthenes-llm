torch.Size([2, 6, 3])
tensor([[[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]],

        [[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]]])

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],
         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],
         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],
         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],

        [[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],
         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],
         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],
         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],
         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],
         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],

        [[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],
         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],
         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],
         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],
         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],
         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],
         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],
         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],
         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],
         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],
         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4260, 0.4252, 0.0000, 0.0000, 0.0000],
         [0.3056, 0.0000, 0.3178, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2575, 0.2573, 0.2418, 0.0000, 0.0000],
         [0.2031, 0.2136, 0.2133, 0.0000, 0.2031, 0.2102]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6041, 0.6459, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3988, 0.0000, 0.4252, 0.0000, 0.0000, 0.0000],
         [0.3056, 0.3182, 0.3178, 0.3085, 0.0000, 0.0000],
         [0.2493, 0.2575, 0.0000, 0.2418, 0.2441, 0.0000],
         [0.2031, 0.2136, 0.2133, 0.2068, 0.2031, 0.2102]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.2327,  0.1055,  0.1098,  0.0913,  0.1549,  0.0521],
         [-0.2396,  0.1015,  0.1057,  0.0902,  0.1501,  0.0518],
         [-0.2323,  0.1004,  0.1045,  0.0885,  0.1481,  0.0507],
         [-0.1344,  0.0502,  0.0523,  0.0470,  0.0753,  0.0272],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0174],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],

        [[-0.2327,  0.1055,  0.1098,  0.0913,  0.1549,  0.0521],
         [-0.2396,  0.1015,  0.1057,  0.0902,  0.1501,  0.0518],
         [-0.2323,  0.1004,  0.1045,  0.0885,  0.1481,  0.0507],
         [-0.1344,  0.0502,  0.0523,  0.0470,  0.0753,  0.0272],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0174],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],
         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],
         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],

        [[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],
         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],
         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],
         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],
         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],
         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],
         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],
         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],
         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],
         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],
         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5500, 0.7000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3537, 0.4475, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.0000, 0.3229, 0.3217, 0.0000, 0.0000],
         [0.2379, 0.0000, 0.2533, 0.2496, 0.2563, 0.0000],
         [0.1760, 0.2144, 0.0000, 0.2146, 0.2198, 0.2105]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.7000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3537, 0.0000, 0.4488, 0.0000, 0.0000, 0.0000],
         [0.2830, 0.0000, 0.3229, 0.0000, 0.0000, 0.0000],
         [0.2379, 0.2529, 0.0000, 0.2496, 0.2563, 0.0000],
         [0.1760, 0.2144, 0.2148, 0.2146, 0.2198, 0.2105]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0320, -0.2465, -0.2400, -0.1684, -0.0549, -0.2388],
         [-0.0337, -0.2816, -0.2742, -0.1929, -0.0627, -0.2733],
         [-0.0330, -0.2810, -0.2736, -0.1926, -0.0625, -0.2728],
         [-0.0181, -0.1395, -0.1358, -0.0953, -0.0311, -0.1351],
         [-0.0108, -0.1907, -0.1857, -0.1326, -0.0421, -0.1872],
         [-0.0263, -0.1566, -0.1524, -0.1060, -0.0351, -0.1506]],

        [[-0.0320, -0.2465, -0.2400, -0.1684, -0.0549, -0.2388],
         [-0.0337, -0.2816, -0.2742, -0.1929, -0.0627, -0.2733],
         [-0.0330, -0.2810, -0.2736, -0.1926, -0.0625, -0.2728],
         [-0.0181, -0.1395, -0.1358, -0.0953, -0.0311, -0.1351],
         [-0.0108, -0.1907, -0.1857, -0.1326, -0.0421, -0.1872],
         [-0.0263, -0.1566, -0.1524, -0.1060, -0.0351, -0.1506]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0320,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0337, -0.2816,    -inf,    -inf,    -inf,    -inf],
         [-0.0330, -0.2810, -0.2736,    -inf,    -inf,    -inf],
         [-0.0181, -0.1395, -0.1358, -0.0953,    -inf,    -inf],
         [-0.0108, -0.1907, -0.1857, -0.1326, -0.0421,    -inf],
         [-0.0263, -0.1566, -0.1524, -0.1060, -0.0351, -0.1506]],

        [[-0.0320,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0337, -0.2816,    -inf,    -inf,    -inf,    -inf],
         [-0.0330, -0.2810, -0.2736,    -inf,    -inf,    -inf],
         [-0.0181, -0.1395, -0.1358, -0.0953,    -inf,    -inf],
         [-0.0108, -0.1907, -0.1857, -0.1326, -0.0421,    -inf],
         [-0.0263, -0.1566, -0.1524, -0.1060, -0.0351, -0.1506]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5437, 0.4563, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3728, 0.3128, 0.3144, 0.0000, 0.0000, 0.0000],
         [0.2642, 0.2425, 0.2431, 0.2502, 0.0000, 0.0000],
         [0.2146, 0.1890, 0.1896, 0.1969, 0.2099, 0.0000],
         [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5437, 0.4563, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3728, 0.3128, 0.3144, 0.0000, 0.0000, 0.0000],
         [0.2642, 0.2425, 0.2431, 0.2502, 0.0000, 0.0000],
         [0.2146, 0.1890, 0.1896, 0.1969, 0.2099, 0.0000],
         [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6796, 0.5704, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3910, 0.3931, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3031, 0.3039, 0.3127, 0.0000, 0.0000],
         [0.2682, 0.2362, 0.2370, 0.0000, 0.2624, 0.0000],
         [0.0000, 0.2007, 0.2012, 0.2080, 0.2186, 0.2015]],

        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6796, 0.5704, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4659, 0.3910, 0.3931, 0.0000, 0.0000, 0.0000],
         [0.3303, 0.0000, 0.3039, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2362, 0.2370, 0.2461, 0.0000, 0.0000],
         [0.2200, 0.2007, 0.2012, 0.2080, 0.0000, 0.2015]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.1974,  0.0342,  0.0335, -0.0081,  0.0126,  0.0009],
         [ 0.2801, -0.0751, -0.0774, -0.0851, -0.0975, -0.0626],
         [ 0.2706, -0.0807, -0.0831, -0.0871, -0.1019, -0.0647],
         [ 0.1717, -0.0365, -0.0378, -0.0465, -0.0509, -0.0334],
         [ 0.0234, -0.1599, -0.1619, -0.0986, -0.1516, -0.0846],
         [ 0.2730,  0.0156,  0.0143, -0.0301, -0.0122, -0.0151]],

        [[ 0.1974,  0.0342,  0.0335, -0.0081,  0.0126,  0.0009],
         [ 0.2801, -0.0751, -0.0774, -0.0851, -0.0975, -0.0626],
         [ 0.2706, -0.0807, -0.0831, -0.0871, -0.1019, -0.0647],
         [ 0.1717, -0.0365, -0.0378, -0.0465, -0.0509, -0.0334],
         [ 0.0234, -0.1599, -0.1619, -0.0986, -0.1516, -0.0846],
         [ 0.2730,  0.0156,  0.0143, -0.0301, -0.0122, -0.0151]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.1974,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.2801, -0.0751,    -inf,    -inf,    -inf,    -inf],
         [ 0.2706, -0.0807, -0.0831,    -inf,    -inf,    -inf],
         [ 0.1717, -0.0365, -0.0378, -0.0465,    -inf,    -inf],
         [ 0.0234, -0.1599, -0.1619, -0.0986, -0.1516,    -inf],
         [ 0.2730,  0.0156,  0.0143, -0.0301, -0.0122, -0.0151]],

        [[ 0.1974,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.2801, -0.0751,    -inf,    -inf,    -inf,    -inf],
         [ 0.2706, -0.0807, -0.0831,    -inf,    -inf,    -inf],
         [ 0.1717, -0.0365, -0.0378, -0.0465,    -inf,    -inf],
         [ 0.0234, -0.1599, -0.1619, -0.0986, -0.1516,    -inf],
         [ 0.2730,  0.0156,  0.0143, -0.0301, -0.0122, -0.0151]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5625, 0.4375, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3908, 0.3048, 0.3043, 0.0000, 0.0000, 0.0000],
         [0.2791, 0.2409, 0.2407, 0.2392, 0.0000, 0.0000],
         [0.2195, 0.1928, 0.1925, 0.2013, 0.1939, 0.0000],
         [0.1958, 0.1632, 0.1631, 0.1580, 0.1601, 0.1597]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5625, 0.4375, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3908, 0.3048, 0.3043, 0.0000, 0.0000, 0.0000],
         [0.2791, 0.2409, 0.2407, 0.2392, 0.0000, 0.0000],
         [0.2195, 0.1928, 0.1925, 0.2013, 0.1939, 0.0000],
         [0.1958, 0.1632, 0.1631, 0.1580, 0.1601, 0.1597]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.7031, 0.5469, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4885, 0.3811, 0.3804, 0.0000, 0.0000, 0.0000],
         [0.3489, 0.0000, 0.0000, 0.2990, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.2406, 0.2517, 0.2424, 0.0000],
         [0.2448, 0.0000, 0.2039, 0.1976, 0.2001, 0.1997]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.7031, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4885, 0.0000, 0.3804, 0.0000, 0.0000, 0.0000],
         [0.3489, 0.3012, 0.3009, 0.0000, 0.0000, 0.0000],
         [0.2743, 0.0000, 0.2406, 0.2517, 0.2424, 0.0000],
         [0.2448, 0.2040, 0.2039, 0.1976, 0.2001, 0.1997]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.0012, -0.0508, -0.0499, -0.0344, -0.0213, -0.0437],
         [-0.1306, -0.2533, -0.2465, -0.1578, -0.0544, -0.2321],
         [-0.1318, -0.2547, -0.2478, -0.1586, -0.0545, -0.2334],
         [-0.0789, -0.1472, -0.1432, -0.0914, -0.0305, -0.1352],
         [-0.1162, -0.2076, -0.2018, -0.1284, -0.0411, -0.1911],
         [-0.0716, -0.1425, -0.1387, -0.0890, -0.0313, -0.1304]],

        [[ 0.0012, -0.0508, -0.0499, -0.0344, -0.0213, -0.0437],
         [-0.1306, -0.2533, -0.2465, -0.1578, -0.0544, -0.2321],
         [-0.1318, -0.2547, -0.2478, -0.1586, -0.0545, -0.2334],
         [-0.0789, -0.1472, -0.1432, -0.0914, -0.0305, -0.1352],
         [-0.1162, -0.2076, -0.2018, -0.1284, -0.0411, -0.1911],
         [-0.0716, -0.1425, -0.1387, -0.0890, -0.0313, -0.1304]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.0012,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1306, -0.2533,    -inf,    -inf,    -inf,    -inf],
         [-0.1318, -0.2547, -0.2478,    -inf,    -inf,    -inf],
         [-0.0789, -0.1472, -0.1432, -0.0914,    -inf,    -inf],
         [-0.1162, -0.2076, -0.2018, -0.1284, -0.0411,    -inf],
         [-0.0716, -0.1425, -0.1387, -0.0890, -0.0313, -0.1304]],

        [[ 0.0012,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1306, -0.2533,    -inf,    -inf,    -inf,    -inf],
         [-0.1318, -0.2547, -0.2478,    -inf,    -inf,    -inf],
         [-0.0789, -0.1472, -0.1432, -0.0914,    -inf,    -inf],
         [-0.1162, -0.2076, -0.2018, -0.1284, -0.0411,    -inf],
         [-0.0716, -0.1425, -0.1387, -0.0890, -0.0313, -0.1304]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5217, 0.4783, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3524, 0.3230, 0.3246, 0.0000, 0.0000, 0.0000],
         [0.2564, 0.2443, 0.2450, 0.2542, 0.0000, 0.0000],
         [0.2031, 0.1904, 0.1911, 0.2013, 0.2141, 0.0000],
         [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5217, 0.4783, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3524, 0.3230, 0.3246, 0.0000, 0.0000, 0.0000],
         [0.2564, 0.2443, 0.2450, 0.2542, 0.0000, 0.0000],
         [0.2031, 0.1904, 0.1911, 0.2013, 0.2141, 0.0000],
         [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6521, 0.5979, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4038, 0.4058, 0.0000, 0.0000, 0.0000],
         [0.3205, 0.3054, 0.3063, 0.3177, 0.0000, 0.0000],
         [0.2538, 0.0000, 0.2389, 0.2516, 0.2677, 0.0000],
         [0.2126, 0.0000, 0.2027, 0.2100, 0.0000, 0.2039]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6521, 0.5979, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4404, 0.4038, 0.4058, 0.0000, 0.0000, 0.0000],
         [0.3205, 0.3054, 0.3063, 0.3177, 0.0000, 0.0000],
         [0.2538, 0.2379, 0.2389, 0.2516, 0.2677, 0.0000],
         [0.2126, 0.2022, 0.2027, 0.2100, 0.2187, 0.2039]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0691,  0.0401,  0.0394,  0.0384,  0.0146,  0.0462],
         [ 0.1479,  0.2531,  0.2514,  0.1405,  0.1492,  0.1664],
         [ 0.1287,  0.2329,  0.2313,  0.1305,  0.1366,  0.1547],
         [ 0.1587,  0.2104,  0.2091,  0.1105,  0.1276,  0.1307],
         [-0.2529, -0.1970, -0.1963, -0.0853, -0.1296, -0.1000],
         [ 0.3459,  0.4120,  0.4097,  0.2103,  0.2532,  0.2483]],

        [[-0.0691,  0.0401,  0.0394,  0.0384,  0.0146,  0.0462],
         [ 0.1479,  0.2531,  0.2514,  0.1405,  0.1492,  0.1664],
         [ 0.1287,  0.2329,  0.2313,  0.1305,  0.1366,  0.1547],
         [ 0.1587,  0.2104,  0.2091,  0.1105,  0.1276,  0.1307],
         [-0.2529, -0.1970, -0.1963, -0.0853, -0.1296, -0.1000],
         [ 0.3459,  0.4120,  0.4097,  0.2103,  0.2532,  0.2483]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0691,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.1479,  0.2531,    -inf,    -inf,    -inf,    -inf],
         [ 0.1287,  0.2329,  0.2313,    -inf,    -inf,    -inf],
         [ 0.1587,  0.2104,  0.2091,  0.1105,    -inf,    -inf],
         [-0.2529, -0.1970, -0.1963, -0.0853, -0.1296,    -inf],
         [ 0.3459,  0.4120,  0.4097,  0.2103,  0.2532,  0.2483]],

        [[-0.0691,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.1479,  0.2531,    -inf,    -inf,    -inf,    -inf],
         [ 0.1287,  0.2329,  0.2313,    -inf,    -inf,    -inf],
         [ 0.1587,  0.2104,  0.2091,  0.1105,    -inf,    -inf],
         [-0.2529, -0.1970, -0.1963, -0.0853, -0.1296,    -inf],
         [ 0.3459,  0.4120,  0.4097,  0.2103,  0.2532,  0.2483]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4814, 0.5186, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3173, 0.3416, 0.3412, 0.0000, 0.0000, 0.0000],
         [0.2475, 0.2567, 0.2565, 0.2392, 0.0000, 0.0000],
         [0.1887, 0.1964, 0.1965, 0.2125, 0.2059, 0.0000],
         [0.1703, 0.1784, 0.1781, 0.1547, 0.1595, 0.1589]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4814, 0.5186, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3173, 0.3416, 0.3412, 0.0000, 0.0000, 0.0000],
         [0.2475, 0.2567, 0.2565, 0.2392, 0.0000, 0.0000],
         [0.1887, 0.1964, 0.1965, 0.2125, 0.2059, 0.0000],
         [0.1703, 0.1784, 0.1781, 0.1547, 0.1595, 0.1589]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4269, 0.4264, 0.0000, 0.0000, 0.0000],
         [0.3094, 0.3209, 0.3206, 0.0000, 0.0000, 0.0000],
         [0.2359, 0.2455, 0.0000, 0.2656, 0.2574, 0.0000],
         [0.2129, 0.2230, 0.0000, 0.0000, 0.0000, 0.0000]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6018, 0.6482, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3966, 0.4269, 0.4264, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.3206, 0.0000, 0.0000, 0.0000],
         [0.2359, 0.2455, 0.2456, 0.2656, 0.0000, 0.0000],
         [0.0000, 0.2230, 0.2227, 0.1934, 0.0000, 0.1987]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.2727, -0.2097, -0.2054, -0.1006, -0.0705, -0.1507],
         [-0.1514, -0.0904, -0.0888, -0.0372, -0.0355, -0.0565],
         [-0.1557, -0.0954, -0.0937, -0.0400, -0.0368, -0.0607],
         [-0.0393, -0.0090, -0.0090,  0.0007, -0.0072,  0.0004],
         [-0.1901, -0.1595, -0.1561, -0.0796, -0.0510, -0.1189],
         [-0.0051,  0.0344,  0.0333,  0.0256,  0.0040,  0.0372]],

        [[-0.2727, -0.2097, -0.2054, -0.1006, -0.0705, -0.1507],
         [-0.1514, -0.0904, -0.0888, -0.0372, -0.0355, -0.0565],
         [-0.1557, -0.0954, -0.0937, -0.0400, -0.0368, -0.0607],
         [-0.0393, -0.0090, -0.0090,  0.0007, -0.0072,  0.0004],
         [-0.1901, -0.1595, -0.1561, -0.0796, -0.0510, -0.1189],
         [-0.0051,  0.0344,  0.0333,  0.0256,  0.0040,  0.0372]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.2727,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1514, -0.0904,    -inf,    -inf,    -inf,    -inf],
         [-0.1557, -0.0954, -0.0937,    -inf,    -inf,    -inf],
         [-0.0393, -0.0090, -0.0090,  0.0007,    -inf,    -inf],
         [-0.1901, -0.1595, -0.1561, -0.0796, -0.0510,    -inf],
         [-0.0051,  0.0344,  0.0333,  0.0256,  0.0040,  0.0372]],

        [[-0.2727,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1514, -0.0904,    -inf,    -inf,    -inf,    -inf],
         [-0.1557, -0.0954, -0.0937,    -inf,    -inf,    -inf],
         [-0.0393, -0.0090, -0.0090,  0.0007,    -inf,    -inf],
         [-0.1901, -0.1595, -0.1561, -0.0796, -0.0510,    -inf],
         [-0.0051,  0.0344,  0.0333,  0.0256,  0.0040,  0.0372]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4892, 0.5108, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3238, 0.3379, 0.3383, 0.0000, 0.0000, 0.0000],
         [0.2456, 0.2509, 0.2509, 0.2526, 0.0000, 0.0000],
         [0.1912, 0.1954, 0.1958, 0.2067, 0.2109, 0.0000],
         [0.1635, 0.1682, 0.1680, 0.1671, 0.1646, 0.1685]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4892, 0.5108, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3238, 0.3379, 0.3383, 0.0000, 0.0000, 0.0000],
         [0.2456, 0.2509, 0.2509, 0.2526, 0.0000, 0.0000],
         [0.1912, 0.1954, 0.1958, 0.2067, 0.2109, 0.0000],
         [0.1635, 0.1682, 0.1680, 0.1671, 0.1646, 0.1685]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4048, 0.4224, 0.4229, 0.0000, 0.0000, 0.0000],
         [0.3070, 0.3136, 0.3136, 0.0000, 0.0000, 0.0000],
         [0.2390, 0.2442, 0.0000, 0.0000, 0.2637, 0.0000],
         [0.2044, 0.2102, 0.0000, 0.2089, 0.0000, 0.2106]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6115, 0.6385, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4048, 0.4224, 0.4229, 0.0000, 0.0000, 0.0000],
         [0.3070, 0.3136, 0.0000, 0.3158, 0.0000, 0.0000],
         [0.2390, 0.2442, 0.2448, 0.2584, 0.0000, 0.0000],
         [0.2044, 0.2102, 0.2101, 0.0000, 0.2058, 0.2106]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0782, -0.1213, -0.1221, -0.0615, -0.1020, -0.0585],
         [-0.0956, -0.2011, -0.1980, -0.1199, -0.0870, -0.1564],
         [-0.0960, -0.1994, -0.1965, -0.1182, -0.0891, -0.1530],
         [-0.0466, -0.1120, -0.1094, -0.0703, -0.0323, -0.0988],
         [-0.0757, -0.1122, -0.1134, -0.0551, -0.1025, -0.0482],
         [-0.0467, -0.1359, -0.1315, -0.0904, -0.0156, -0.1370]],

        [[-0.0782, -0.1213, -0.1221, -0.0615, -0.1020, -0.0585],
         [-0.0956, -0.2011, -0.1980, -0.1199, -0.0870, -0.1564],
         [-0.0960, -0.1994, -0.1965, -0.1182, -0.0891, -0.1530],
         [-0.0466, -0.1120, -0.1094, -0.0703, -0.0323, -0.0988],
         [-0.0757, -0.1122, -0.1134, -0.0551, -0.1025, -0.0482],
         [-0.0467, -0.1359, -0.1315, -0.0904, -0.0156, -0.1370]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0782,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0956, -0.2011,    -inf,    -inf,    -inf,    -inf],
         [-0.0960, -0.1994, -0.1965,    -inf,    -inf,    -inf],
         [-0.0466, -0.1120, -0.1094, -0.0703,    -inf,    -inf],
         [-0.0757, -0.1122, -0.1134, -0.0551, -0.1025,    -inf],
         [-0.0467, -0.1359, -0.1315, -0.0904, -0.0156, -0.1370]],

        [[-0.0782,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0956, -0.2011,    -inf,    -inf,    -inf,    -inf],
         [-0.0960, -0.1994, -0.1965,    -inf,    -inf,    -inf],
         [-0.0466, -0.1120, -0.1094, -0.0703,    -inf,    -inf],
         [-0.0757, -0.1122, -0.1134, -0.0551, -0.1025,    -inf],
         [-0.0467, -0.1359, -0.1315, -0.0904, -0.0156, -0.1370]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5186, 0.4814, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3495, 0.3249, 0.3256, 0.0000, 0.0000, 0.0000],
         [0.2568, 0.2452, 0.2456, 0.2525, 0.0000, 0.0000],
         [0.2023, 0.1971, 0.1969, 0.2052, 0.1985, 0.0000],
         [0.1721, 0.1616, 0.1621, 0.1669, 0.1759, 0.1615]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5186, 0.4814, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3495, 0.3249, 0.3256, 0.0000, 0.0000, 0.0000],
         [0.2568, 0.2452, 0.2456, 0.2525, 0.0000, 0.0000],
         [0.2023, 0.1971, 0.1969, 0.2052, 0.1985, 0.0000],
         [0.1721, 0.1616, 0.1621, 0.1669, 0.1759, 0.1615]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6483, 0.6017, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3209, 0.0000, 0.0000, 0.3156, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.2462, 0.2565, 0.2481, 0.0000],
         [0.2151, 0.2020, 0.2026, 0.2086, 0.2199, 0.2018]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.6017, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4369, 0.0000, 0.4070, 0.0000, 0.0000, 0.0000],
         [0.3209, 0.3064, 0.3070, 0.3156, 0.0000, 0.0000],
         [0.2528, 0.0000, 0.0000, 0.2565, 0.0000, 0.0000],
         [0.2151, 0.0000, 0.2026, 0.0000, 0.2199, 0.2018]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.2100, -0.1944, -0.1907, -0.0993, -0.0702, -0.1429],
         [-0.3174, -0.3055, -0.3003, -0.1563, -0.1219, -0.2187],
         [-0.3087, -0.2976, -0.2925, -0.1522, -0.1191, -0.2128],
         [-0.1909, -0.1836, -0.1805, -0.0939, -0.0732, -0.1315],
         [-0.0648, -0.0710, -0.0703, -0.0365, -0.0367, -0.0466],
         [-0.2861, -0.2708, -0.2659, -0.1384, -0.1036, -0.1960]],

        [[-0.2100, -0.1944, -0.1907, -0.0993, -0.0702, -0.1429],
         [-0.3174, -0.3055, -0.3003, -0.1563, -0.1219, -0.2187],
         [-0.3087, -0.2976, -0.2925, -0.1522, -0.1191, -0.2128],
         [-0.1909, -0.1836, -0.1805, -0.0939, -0.0732, -0.1315],
         [-0.0648, -0.0710, -0.0703, -0.0365, -0.0367, -0.0466],
         [-0.2861, -0.2708, -0.2659, -0.1384, -0.1036, -0.1960]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.2100,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.3174, -0.3055,    -inf,    -inf,    -inf,    -inf],
         [-0.3087, -0.2976, -0.2925,    -inf,    -inf,    -inf],
         [-0.1909, -0.1836, -0.1805, -0.0939,    -inf,    -inf],
         [-0.0648, -0.0710, -0.0703, -0.0365, -0.0367,    -inf],
         [-0.2861, -0.2708, -0.2659, -0.1384, -0.1036, -0.1960]],

        [[-0.2100,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.3174, -0.3055,    -inf,    -inf,    -inf,    -inf],
         [-0.3087, -0.2976, -0.2925,    -inf,    -inf,    -inf],
         [-0.1909, -0.1836, -0.1805, -0.0939,    -inf,    -inf],
         [-0.0648, -0.0710, -0.0703, -0.0365, -0.0367,    -inf],
         [-0.2861, -0.2708, -0.2659, -0.1384, -0.1036, -0.1960]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4979, 0.5021, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3312, 0.3338, 0.3350, 0.0000, 0.0000, 0.0000],
         [0.2449, 0.2461, 0.2467, 0.2623, 0.0000, 0.0000],
         [0.1987, 0.1979, 0.1980, 0.2027, 0.2027, 0.0000],
         [0.1578, 0.1595, 0.1600, 0.1751, 0.1795, 0.1681]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4979, 0.5021, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3312, 0.3338, 0.3350, 0.0000, 0.0000, 0.0000],
         [0.2449, 0.2461, 0.2467, 0.2623, 0.0000, 0.0000],
         [0.1987, 0.1979, 0.1980, 0.2027, 0.2027, 0.0000],
         [0.1578, 0.1595, 0.1600, 0.1751, 0.1795, 0.1681]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6224, 0.6276, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4140, 0.4173, 0.4188, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3077, 0.3084, 0.3278, 0.0000, 0.0000],
         [0.2484, 0.2473, 0.2475, 0.2534, 0.0000, 0.0000],
         [0.1972, 0.0000, 0.2000, 0.0000, 0.0000, 0.2102]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6224, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4140, 0.4173, 0.4188, 0.0000, 0.0000, 0.0000],
         [0.3061, 0.3077, 0.3084, 0.3278, 0.0000, 0.0000],
         [0.2484, 0.0000, 0.0000, 0.2534, 0.0000, 0.0000],
         [0.1972, 0.1993, 0.0000, 0.2189, 0.2244, 0.2102]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0004,  0.0622,  0.0547,  0.0604, -0.0950,  0.1337],
         [ 0.0340,  0.1598,  0.1524,  0.1162, -0.0243,  0.1925],
         [ 0.0289,  0.1450,  0.1376,  0.1076, -0.0342,  0.1831],
         [ 0.0360,  0.1328,  0.1292,  0.0879,  0.0284,  0.1266],
         [-0.0711, -0.1625, -0.1671, -0.0774, -0.2045, -0.0384],
         [ 0.0858,  0.2799,  0.2757,  0.1742,  0.1222,  0.2241]],

        [[-0.0004,  0.0622,  0.0547,  0.0604, -0.0950,  0.1337],
         [ 0.0340,  0.1598,  0.1524,  0.1162, -0.0243,  0.1925],
         [ 0.0289,  0.1450,  0.1376,  0.1076, -0.0342,  0.1831],
         [ 0.0360,  0.1328,  0.1292,  0.0879,  0.0284,  0.1266],
         [-0.0711, -0.1625, -0.1671, -0.0774, -0.2045, -0.0384],
         [ 0.0858,  0.2799,  0.2757,  0.1742,  0.1222,  0.2241]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0004,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0340,  0.1598,    -inf,    -inf,    -inf,    -inf],
         [ 0.0289,  0.1450,  0.1376,    -inf,    -inf,    -inf],
         [ 0.0360,  0.1328,  0.1292,  0.0879,    -inf,    -inf],
         [-0.0711, -0.1625, -0.1671, -0.0774, -0.2045,    -inf],
         [ 0.0858,  0.2799,  0.2757,  0.1742,  0.1222,  0.2241]],

        [[-0.0004,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0340,  0.1598,    -inf,    -inf,    -inf,    -inf],
         [ 0.0289,  0.1450,  0.1376,    -inf,    -inf,    -inf],
         [ 0.0360,  0.1328,  0.1292,  0.0879,    -inf,    -inf],
         [-0.0711, -0.1625, -0.1671, -0.0774, -0.2045,    -inf],
         [ 0.0858,  0.2799,  0.2757,  0.1742,  0.1222,  0.2241]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4778, 0.5222, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3159, 0.3429, 0.3412, 0.0000, 0.0000, 0.0000],
         [0.2394, 0.2564, 0.2558, 0.2484, 0.0000, 0.0000],
         [0.2093, 0.1962, 0.1956, 0.2084, 0.1905, 0.0000],
         [0.1542, 0.1769, 0.1764, 0.1642, 0.1582, 0.1701]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4778, 0.5222, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3159, 0.3429, 0.3412, 0.0000, 0.0000, 0.0000],
         [0.2394, 0.2564, 0.2558, 0.2484, 0.0000, 0.0000],
         [0.2093, 0.1962, 0.1956, 0.2084, 0.1905, 0.0000],
         [0.1542, 0.1769, 0.1764, 0.1642, 0.1582, 0.1701]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5972, 0.6528, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3949, 0.4287, 0.4264, 0.0000, 0.0000, 0.0000],
         [0.2993, 0.3205, 0.3197, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2453, 0.2445, 0.2605, 0.2381, 0.0000],
         [0.1928, 0.2211, 0.0000, 0.2052, 0.1978, 0.2126]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5972, 0.6528, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3949, 0.4287, 0.4264, 0.0000, 0.0000, 0.0000],
         [0.2993, 0.3205, 0.3197, 0.3105, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.2381, 0.0000],
         [0.0000, 0.2211, 0.2205, 0.2052, 0.0000, 0.0000]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[0.4540, 0.6118, 0.6008, 0.3432, 0.2353, 0.4709],
         [0.7106, 0.9608, 0.9481, 0.5270, 0.4513, 0.6858],
         [0.7063, 0.9551, 0.9426, 0.5236, 0.4509, 0.6802],
         [0.3865, 0.5227, 0.5158, 0.2864, 0.2472, 0.3719],
         [0.4301, 0.5834, 0.5779, 0.3136, 0.3171, 0.3880],
         [0.4540, 0.6131, 0.6038, 0.3394, 0.2672, 0.4513]],

        [[0.4540, 0.6118, 0.6008, 0.3432, 0.2353, 0.4709],
         [0.7106, 0.9608, 0.9481, 0.5270, 0.4513, 0.6858],
         [0.7063, 0.9551, 0.9426, 0.5236, 0.4509, 0.6802],
         [0.3865, 0.5227, 0.5158, 0.2864, 0.2472, 0.3719],
         [0.4301, 0.5834, 0.5779, 0.3136, 0.3171, 0.3880],
         [0.4540, 0.6131, 0.6038, 0.3394, 0.2672, 0.4513]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[0.4540,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.7106, 0.9608,   -inf,   -inf,   -inf,   -inf],
         [0.7063, 0.9551, 0.9426,   -inf,   -inf,   -inf],
         [0.3865, 0.5227, 0.5158, 0.2864,   -inf,   -inf],
         [0.4301, 0.5834, 0.5779, 0.3136, 0.3171,   -inf],
         [0.4540, 0.6131, 0.6038, 0.3394, 0.2672, 0.4513]],

        [[0.4540,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.7106, 0.9608,   -inf,   -inf,   -inf,   -inf],
         [0.7063, 0.9551, 0.9426,   -inf,   -inf,   -inf],
         [0.3865, 0.5227, 0.5158, 0.2864,   -inf,   -inf],
         [0.4301, 0.5834, 0.5779, 0.3136, 0.3171,   -inf],
         [0.4540, 0.6131, 0.6038, 0.3394, 0.2672, 0.4513]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4559, 0.5441, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2964, 0.3534, 0.3503, 0.0000, 0.0000, 0.0000],
         [0.2422, 0.2667, 0.2654, 0.2257, 0.0000, 0.0000],
         [0.1973, 0.2199, 0.2190, 0.1817, 0.1821, 0.0000],
         [0.1659, 0.1857, 0.1845, 0.1530, 0.1454, 0.1656]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4559, 0.5441, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2964, 0.3534, 0.3503, 0.0000, 0.0000, 0.0000],
         [0.2422, 0.2667, 0.2654, 0.2257, 0.0000, 0.0000],
         [0.1973, 0.2199, 0.2190, 0.1817, 0.1821, 0.0000],
         [0.1659, 0.1857, 0.1845, 0.1530, 0.1454, 0.1656]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5698, 0.6802, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3705, 0.4417, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3334, 0.3318, 0.2821, 0.0000, 0.0000],
         [0.2466, 0.2748, 0.2738, 0.2271, 0.2277, 0.0000],
         [0.2074, 0.2321, 0.2306, 0.1912, 0.1817, 0.2070]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5698, 0.6802, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3705, 0.0000, 0.4378, 0.0000, 0.0000, 0.0000],
         [0.3028, 0.3334, 0.3318, 0.2821, 0.0000, 0.0000],
         [0.0000, 0.2748, 0.0000, 0.2271, 0.2277, 0.0000],
         [0.2074, 0.2321, 0.0000, 0.1912, 0.0000, 0.0000]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0607, -0.0454, -0.0454, -0.0187, -0.0336, -0.0202],
         [-0.0375, -0.0586, -0.0615, -0.0228, -0.0951,  0.0020],
         [-0.0390, -0.0609, -0.0638, -0.0237, -0.0986,  0.0021],
         [-0.0097, -0.0233, -0.0248, -0.0089, -0.0444,  0.0044],
         [-0.0549, -0.0837, -0.0876, -0.0326, -0.1340,  0.0020],
         [ 0.0028, -0.0044, -0.0049, -0.0015, -0.0142,  0.0040]],

        [[-0.0607, -0.0454, -0.0454, -0.0187, -0.0336, -0.0202],
         [-0.0375, -0.0586, -0.0615, -0.0228, -0.0951,  0.0020],
         [-0.0390, -0.0609, -0.0638, -0.0237, -0.0986,  0.0021],
         [-0.0097, -0.0233, -0.0248, -0.0089, -0.0444,  0.0044],
         [-0.0549, -0.0837, -0.0876, -0.0326, -0.1340,  0.0020],
         [ 0.0028, -0.0044, -0.0049, -0.0015, -0.0142,  0.0040]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0607,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0375, -0.0586,    -inf,    -inf,    -inf,    -inf],
         [-0.0390, -0.0609, -0.0638,    -inf,    -inf,    -inf],
         [-0.0097, -0.0233, -0.0248, -0.0089,    -inf,    -inf],
         [-0.0549, -0.0837, -0.0876, -0.0326, -0.1340,    -inf],
         [ 0.0028, -0.0044, -0.0049, -0.0015, -0.0142,  0.0040]],

        [[-0.0607,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0375, -0.0586,    -inf,    -inf,    -inf,    -inf],
         [-0.0390, -0.0609, -0.0638,    -inf,    -inf,    -inf],
         [-0.0097, -0.0233, -0.0248, -0.0089,    -inf,    -inf],
         [-0.0549, -0.0837, -0.0876, -0.0326, -0.1340,    -inf],
         [ 0.0028, -0.0044, -0.0049, -0.0015, -0.0142,  0.0040]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5037, 0.4963, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3370, 0.3318, 0.3312, 0.0000, 0.0000, 0.0000],
         [0.2512, 0.2488, 0.2486, 0.2514, 0.0000, 0.0000],
         [0.2033, 0.1992, 0.1987, 0.2065, 0.1923, 0.0000],
         [0.1674, 0.1665, 0.1664, 0.1668, 0.1654, 0.1675]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5037, 0.4963, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3370, 0.3318, 0.3312, 0.0000, 0.0000, 0.0000],
         [0.2512, 0.2488, 0.2486, 0.2514, 0.0000, 0.0000],
         [0.2033, 0.1992, 0.1987, 0.2065, 0.1923, 0.0000],
         [0.1674, 0.1665, 0.1664, 0.1668, 0.1654, 0.1675]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.6203, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4213, 0.4148, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3140, 0.3110, 0.3107, 0.3142, 0.0000, 0.0000],
         [0.2541, 0.2490, 0.2483, 0.0000, 0.2403, 0.0000],
         [0.2092, 0.2081, 0.2080, 0.2086, 0.2067, 0.2094]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6297, 0.6203, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4213, 0.4148, 0.4139, 0.0000, 0.0000, 0.0000],
         [0.3140, 0.3110, 0.3107, 0.3142, 0.0000, 0.0000],
         [0.2541, 0.2490, 0.2483, 0.0000, 0.2403, 0.0000],
         [0.2092, 0.2081, 0.2080, 0.2086, 0.2067, 0.2094]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[0.3322, 0.3854, 0.3839, 0.1936, 0.2487, 0.2232],
         [0.4227, 0.5480, 0.5454, 0.2844, 0.3456, 0.3301],
         [0.4135, 0.5338, 0.5313, 0.2767, 0.3369, 0.3211],
         [0.2387, 0.3235, 0.3219, 0.1699, 0.2022, 0.1976],
         [0.1330, 0.1281, 0.1278, 0.0602, 0.0863, 0.0684],
         [0.3422, 0.4798, 0.4773, 0.2541, 0.2981, 0.2962]],

        [[0.3322, 0.3854, 0.3839, 0.1936, 0.2487, 0.2232],
         [0.4227, 0.5480, 0.5454, 0.2844, 0.3456, 0.3301],
         [0.4135, 0.5338, 0.5313, 0.2767, 0.3369, 0.3211],
         [0.2387, 0.3235, 0.3219, 0.1699, 0.2022, 0.1976],
         [0.1330, 0.1281, 0.1278, 0.0602, 0.0863, 0.0684],
         [0.3422, 0.4798, 0.4773, 0.2541, 0.2981, 0.2962]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[0.3322,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.4227, 0.5480,   -inf,   -inf,   -inf,   -inf],
         [0.4135, 0.5338, 0.5313,   -inf,   -inf,   -inf],
         [0.2387, 0.3235, 0.3219, 0.1699,   -inf,   -inf],
         [0.1330, 0.1281, 0.1278, 0.0602, 0.0863,   -inf],
         [0.3422, 0.4798, 0.4773, 0.2541, 0.2981, 0.2962]],

        [[0.3322,   -inf,   -inf,   -inf,   -inf,   -inf],
         [0.4227, 0.5480,   -inf,   -inf,   -inf,   -inf],
         [0.4135, 0.5338, 0.5313,   -inf,   -inf,   -inf],
         [0.2387, 0.3235, 0.3219, 0.1699,   -inf,   -inf],
         [0.1330, 0.1281, 0.1278, 0.0602, 0.0863,   -inf],
         [0.3422, 0.4798, 0.4773, 0.2541, 0.2981, 0.2962]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4779, 0.5221, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3149, 0.3429, 0.3423, 0.0000, 0.0000, 0.0000],
         [0.2454, 0.2606, 0.2603, 0.2337, 0.0000, 0.0000],
         [0.2037, 0.2030, 0.2029, 0.1934, 0.1970, 0.0000],
         [0.1645, 0.1813, 0.1810, 0.1546, 0.1594, 0.1592]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4779, 0.5221, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3149, 0.3429, 0.3423, 0.0000, 0.0000, 0.0000],
         [0.2454, 0.2606, 0.2603, 0.2337, 0.0000, 0.0000],
         [0.2037, 0.2030, 0.2029, 0.1934, 0.1970, 0.0000],
         [0.1645, 0.1813, 0.1810, 0.1546, 0.1594, 0.1592]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5973, 0.6527, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3936, 0.4286, 0.4278, 0.0000, 0.0000, 0.0000],
         [0.3068, 0.3257, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2546, 0.2537, 0.2536, 0.2418, 0.2463, 0.0000],
         [0.2056, 0.2266, 0.2262, 0.1932, 0.1993, 0.1990]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5973, 0.6527, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4286, 0.4278, 0.0000, 0.0000, 0.0000],
         [0.3068, 0.3257, 0.3253, 0.0000, 0.0000, 0.0000],
         [0.2546, 0.2537, 0.2536, 0.2418, 0.2463, 0.0000],
         [0.2056, 0.2266, 0.2262, 0.1932, 0.1993, 0.1990]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.0691,  0.1127,  0.1115,  0.0634,  0.0576,  0.0795],
         [-0.1102, -0.1760, -0.1738, -0.0994, -0.0849, -0.1271],
         [-0.1085, -0.1732, -0.1710, -0.0978, -0.0834, -0.1252],
         [-0.0850, -0.1365, -0.1348, -0.0770, -0.0669, -0.0980],
         [-0.0482, -0.0744, -0.0733, -0.0423, -0.0325, -0.0558],
         [-0.1035, -0.1674, -0.1655, -0.0943, -0.0836, -0.1193]],

        [[ 0.0691,  0.1127,  0.1115,  0.0634,  0.0576,  0.0795],
         [-0.1102, -0.1760, -0.1738, -0.0994, -0.0849, -0.1271],
         [-0.1085, -0.1732, -0.1710, -0.0978, -0.0834, -0.1252],
         [-0.0850, -0.1365, -0.1348, -0.0770, -0.0669, -0.0980],
         [-0.0482, -0.0744, -0.0733, -0.0423, -0.0325, -0.0558],
         [-0.1035, -0.1674, -0.1655, -0.0943, -0.0836, -0.1193]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.0691,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1102, -0.1760,    -inf,    -inf,    -inf,    -inf],
         [-0.1085, -0.1732, -0.1710,    -inf,    -inf,    -inf],
         [-0.0850, -0.1365, -0.1348, -0.0770,    -inf,    -inf],
         [-0.0482, -0.0744, -0.0733, -0.0423, -0.0325,    -inf],
         [-0.1035, -0.1674, -0.1655, -0.0943, -0.0836, -0.1193]],

        [[ 0.0691,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1102, -0.1760,    -inf,    -inf,    -inf,    -inf],
         [-0.1085, -0.1732, -0.1710,    -inf,    -inf,    -inf],
         [-0.0850, -0.1365, -0.1348, -0.0770,    -inf,    -inf],
         [-0.0482, -0.0744, -0.0733, -0.0423, -0.0325,    -inf],
         [-0.1035, -0.1674, -0.1655, -0.0943, -0.0836, -0.1193]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5116, 0.4884, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3434, 0.3280, 0.3286, 0.0000, 0.0000, 0.0000],
         [0.2541, 0.2450, 0.2453, 0.2556, 0.0000, 0.0000],
         [0.2008, 0.1971, 0.1973, 0.2017, 0.2031, 0.0000],
         [0.1688, 0.1614, 0.1616, 0.1699, 0.1712, 0.1670]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5116, 0.4884, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3434, 0.3280, 0.3286, 0.0000, 0.0000, 0.0000],
         [0.2541, 0.2450, 0.2453, 0.2556, 0.0000, 0.0000],
         [0.2008, 0.1971, 0.1973, 0.2017, 0.2031, 0.0000],
         [0.1688, 0.1614, 0.1616, 0.1699, 0.1712, 0.1670]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6395, 0.6105, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4293, 0.4101, 0.4107, 0.0000, 0.0000, 0.0000],
         [0.3176, 0.0000, 0.3066, 0.3194, 0.0000, 0.0000],
         [0.0000, 0.2464, 0.2466, 0.2521, 0.0000, 0.0000],
         [0.2111, 0.2017, 0.0000, 0.2124, 0.2141, 0.2087]],

        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6395, 0.6105, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4293, 0.4101, 0.4107, 0.0000, 0.0000, 0.0000],
         [0.3176, 0.3063, 0.3066, 0.3194, 0.0000, 0.0000],
         [0.2510, 0.2464, 0.2466, 0.0000, 0.2538, 0.0000],
         [0.0000, 0.0000, 0.2020, 0.0000, 0.2141, 0.2087]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.1727,  0.0212,  0.0237,  0.0337,  0.0620,  0.0089],
         [-0.0830, -0.1038, -0.1041, -0.0513, -0.0794, -0.0529],
         [-0.0806, -0.0926, -0.0928, -0.0450, -0.0692, -0.0472],
         [-0.0313, -0.1006, -0.1015, -0.0557, -0.0888, -0.0508],
         [-0.0156,  0.1354,  0.1373,  0.0821,  0.1334,  0.0677],
         [-0.0580, -0.2108, -0.2128, -0.1177, -0.1879, -0.1063]],

        [[-0.1727,  0.0212,  0.0237,  0.0337,  0.0620,  0.0089],
         [-0.0830, -0.1038, -0.1041, -0.0513, -0.0794, -0.0529],
         [-0.0806, -0.0926, -0.0928, -0.0450, -0.0692, -0.0472],
         [-0.0313, -0.1006, -0.1015, -0.0557, -0.0888, -0.0508],
         [-0.0156,  0.1354,  0.1373,  0.0821,  0.1334,  0.0677],
         [-0.0580, -0.2108, -0.2128, -0.1177, -0.1879, -0.1063]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.1727,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0830, -0.1038,    -inf,    -inf,    -inf,    -inf],
         [-0.0806, -0.0926, -0.0928,    -inf,    -inf,    -inf],
         [-0.0313, -0.1006, -0.1015, -0.0557,    -inf,    -inf],
         [-0.0156,  0.1354,  0.1373,  0.0821,  0.1334,    -inf],
         [-0.0580, -0.2108, -0.2128, -0.1177, -0.1879, -0.1063]],

        [[-0.1727,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.0830, -0.1038,    -inf,    -inf,    -inf,    -inf],
         [-0.0806, -0.0926, -0.0928,    -inf,    -inf,    -inf],
         [-0.0313, -0.1006, -0.1015, -0.0557,    -inf,    -inf],
         [-0.0156,  0.1354,  0.1373,  0.0821,  0.1334,    -inf],
         [-0.0580, -0.2108, -0.2128, -0.1177, -0.1879, -0.1063]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5037, 0.4963, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3352, 0.3324, 0.3324, 0.0000, 0.0000, 0.0000],
         [0.2573, 0.2450, 0.2448, 0.2529, 0.0000, 0.0000],
         [0.1849, 0.2057, 0.2060, 0.1981, 0.2054, 0.0000],
         [0.1776, 0.1594, 0.1592, 0.1702, 0.1620, 0.1716]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5037, 0.4963, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3352, 0.3324, 0.3324, 0.0000, 0.0000, 0.0000],
         [0.2573, 0.2450, 0.2448, 0.2529, 0.0000, 0.0000],
         [0.1849, 0.2057, 0.2060, 0.1981, 0.2054, 0.0000],
         [0.1776, 0.1594, 0.1592, 0.1702, 0.1620, 0.1716]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6296, 0.6204, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3216, 0.0000, 0.3060, 0.3161, 0.0000, 0.0000],
         [0.2311, 0.2571, 0.2575, 0.2476, 0.2568, 0.0000],
         [0.2220, 0.1992, 0.1990, 0.2128, 0.2025, 0.0000]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6296, 0.6204, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4190, 0.4155, 0.4155, 0.0000, 0.0000, 0.0000],
         [0.3216, 0.3062, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2311, 0.2571, 0.2575, 0.2476, 0.0000, 0.0000],
         [0.2220, 0.1992, 0.1990, 0.2128, 0.0000, 0.2145]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.2897,  0.0694,  0.0753, -0.0193,  0.1600, -0.0707],
         [ 0.2420, -0.0008,  0.0063, -0.0595,  0.1337, -0.1258],
         [ 0.2409,  0.0036,  0.0106, -0.0560,  0.1331, -0.1201],
         [ 0.1091, -0.0243, -0.0201, -0.0445,  0.0602, -0.0838],
         [ 0.1533,  0.0828,  0.0841,  0.0237,  0.0847,  0.0149],
         [ 0.1299, -0.0662, -0.0598, -0.0804,  0.0717, -0.1421]],

        [[ 0.2897,  0.0694,  0.0753, -0.0193,  0.1600, -0.0707],
         [ 0.2420, -0.0008,  0.0063, -0.0595,  0.1337, -0.1258],
         [ 0.2409,  0.0036,  0.0106, -0.0560,  0.1331, -0.1201],
         [ 0.1091, -0.0243, -0.0201, -0.0445,  0.0602, -0.0838],
         [ 0.1533,  0.0828,  0.0841,  0.0237,  0.0847,  0.0149],
         [ 0.1299, -0.0662, -0.0598, -0.0804,  0.0717, -0.1421]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.2897,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.2420, -0.0008,    -inf,    -inf,    -inf,    -inf],
         [ 0.2409,  0.0036,  0.0106,    -inf,    -inf,    -inf],
         [ 0.1091, -0.0243, -0.0201, -0.0445,    -inf,    -inf],
         [ 0.1533,  0.0828,  0.0841,  0.0237,  0.0847,    -inf],
         [ 0.1299, -0.0662, -0.0598, -0.0804,  0.0717, -0.1421]],

        [[ 0.2897,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.2420, -0.0008,    -inf,    -inf,    -inf,    -inf],
         [ 0.2409,  0.0036,  0.0106,    -inf,    -inf,    -inf],
         [ 0.1091, -0.0243, -0.0201, -0.0445,    -inf,    -inf],
         [ 0.1533,  0.0828,  0.0841,  0.0237,  0.0847,    -inf],
         [ 0.1299, -0.0662, -0.0598, -0.0804,  0.0717, -0.1421]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5428, 0.4572, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3710, 0.3137, 0.3153, 0.0000, 0.0000, 0.0000],
         [0.2688, 0.2446, 0.2454, 0.2412, 0.0000, 0.0000],
         [0.2097, 0.1995, 0.1997, 0.1913, 0.1998, 0.0000],
         [0.1855, 0.1615, 0.1622, 0.1598, 0.1780, 0.1530]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5428, 0.4572, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3710, 0.3137, 0.3153, 0.0000, 0.0000, 0.0000],
         [0.2688, 0.2446, 0.2454, 0.2412, 0.0000, 0.0000],
         [0.2097, 0.1995, 0.1997, 0.1913, 0.1998, 0.0000],
         [0.1855, 0.1615, 0.1622, 0.1598, 0.1780, 0.1530]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6785, 0.5715, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3921, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3360, 0.3058, 0.3067, 0.0000, 0.0000, 0.0000],
         [0.2621, 0.0000, 0.2496, 0.2392, 0.2497, 0.0000],
         [0.2318, 0.2018, 0.2027, 0.1998, 0.2225, 0.1913]],

        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6785, 0.5715, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3921, 0.3941, 0.0000, 0.0000, 0.0000],
         [0.3360, 0.3058, 0.3067, 0.0000, 0.0000, 0.0000],
         [0.2621, 0.2494, 0.2496, 0.0000, 0.0000, 0.0000],
         [0.2318, 0.2018, 0.2027, 0.1998, 0.2225, 0.1913]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0382, -0.0334, -0.0329, -0.0164, -0.0141, -0.0229],
         [ 0.0647,  0.0941,  0.0920,  0.0549,  0.0281,  0.0786],
         [ 0.0630,  0.0870,  0.0851,  0.0501,  0.0268,  0.0716],
         [ 0.0518,  0.0848,  0.0828,  0.0507,  0.0236,  0.0729],
         [ 0.0137, -0.0657, -0.0634, -0.0501, -0.0038, -0.0741],
         [ 0.0702,  0.1555,  0.1514,  0.0980,  0.0366,  0.1418]],

        [[-0.0382, -0.0334, -0.0329, -0.0164, -0.0141, -0.0229],
         [ 0.0647,  0.0941,  0.0920,  0.0549,  0.0281,  0.0786],
         [ 0.0630,  0.0870,  0.0851,  0.0501,  0.0268,  0.0716],
         [ 0.0518,  0.0848,  0.0828,  0.0507,  0.0236,  0.0729],
         [ 0.0137, -0.0657, -0.0634, -0.0501, -0.0038, -0.0741],
         [ 0.0702,  0.1555,  0.1514,  0.0980,  0.0366,  0.1418]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0382,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0647,  0.0941,    -inf,    -inf,    -inf,    -inf],
         [ 0.0630,  0.0870,  0.0851,    -inf,    -inf,    -inf],
         [ 0.0518,  0.0848,  0.0828,  0.0507,    -inf,    -inf],
         [ 0.0137, -0.0657, -0.0634, -0.0501, -0.0038,    -inf],
         [ 0.0702,  0.1555,  0.1514,  0.0980,  0.0366,  0.1418]],

        [[-0.0382,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0647,  0.0941,    -inf,    -inf,    -inf,    -inf],
         [ 0.0630,  0.0870,  0.0851,    -inf,    -inf,    -inf],
         [ 0.0518,  0.0848,  0.0828,  0.0507,    -inf,    -inf],
         [ 0.0137, -0.0657, -0.0634, -0.0501, -0.0038,    -inf],
         [ 0.0702,  0.1555,  0.1514,  0.0980,  0.0366,  0.1418]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4948, 0.5052, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3297, 0.3354, 0.3349, 0.0000, 0.0000, 0.0000],
         [0.2472, 0.2531, 0.2527, 0.2470, 0.0000, 0.0000],
         [0.2068, 0.1955, 0.1958, 0.1977, 0.2042, 0.0000],
         [0.1621, 0.1722, 0.1717, 0.1653, 0.1583, 0.1705]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4948, 0.5052, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3297, 0.3354, 0.3349, 0.0000, 0.0000, 0.0000],
         [0.2472, 0.2531, 0.2527, 0.2470, 0.0000, 0.0000],
         [0.2068, 0.1955, 0.1958, 0.1977, 0.2042, 0.0000],
         [0.1621, 0.1722, 0.1717, 0.1653, 0.1583, 0.1705]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6185, 0.6315, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4121, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3090, 0.3163, 0.3159, 0.3088, 0.0000, 0.0000],
         [0.2585, 0.2444, 0.2448, 0.2471, 0.2553, 0.0000],
         [0.2026, 0.0000, 0.2146, 0.2066, 0.1978, 0.2131]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6185, 0.6315, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4121, 0.4192, 0.4186, 0.0000, 0.0000, 0.0000],
         [0.3090, 0.3163, 0.0000, 0.3088, 0.0000, 0.0000],
         [0.2585, 0.2444, 0.0000, 0.2471, 0.2553, 0.0000],
         [0.2026, 0.2152, 0.2146, 0.2066, 0.1978, 0.2131]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.0076, -0.1478, -0.1399, -0.1166,  0.0415, -0.1965],
         [ 0.0862, -0.1828, -0.1713, -0.1612,  0.0828, -0.2754],
         [ 0.0826, -0.1814, -0.1701, -0.1593,  0.0810, -0.2720],
         [ 0.0633, -0.0953, -0.0889, -0.0881,  0.0507, -0.1513],
         [-0.0058, -0.1051, -0.0998, -0.0805,  0.0249, -0.1350],
         [ 0.1006, -0.1158, -0.1075, -0.1123,  0.0714, -0.1939]],

        [[ 0.0076, -0.1478, -0.1399, -0.1166,  0.0415, -0.1965],
         [ 0.0862, -0.1828, -0.1713, -0.1612,  0.0828, -0.2754],
         [ 0.0826, -0.1814, -0.1701, -0.1593,  0.0810, -0.2720],
         [ 0.0633, -0.0953, -0.0889, -0.0881,  0.0507, -0.1513],
         [-0.0058, -0.1051, -0.0998, -0.0805,  0.0249, -0.1350],
         [ 0.1006, -0.1158, -0.1075, -0.1123,  0.0714, -0.1939]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.0076,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0862, -0.1828,    -inf,    -inf,    -inf,    -inf],
         [ 0.0826, -0.1814, -0.1701,    -inf,    -inf,    -inf],
         [ 0.0633, -0.0953, -0.0889, -0.0881,    -inf,    -inf],
         [-0.0058, -0.1051, -0.0998, -0.0805,  0.0249,    -inf],
         [ 0.1006, -0.1158, -0.1075, -0.1123,  0.0714, -0.1939]],

        [[ 0.0076,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0862, -0.1828,    -inf,    -inf,    -inf,    -inf],
         [ 0.0826, -0.1814, -0.1701,    -inf,    -inf,    -inf],
         [ 0.0633, -0.0953, -0.0889, -0.0881,    -inf,    -inf],
         [-0.0058, -0.1051, -0.0998, -0.0805,  0.0249,    -inf],
         [ 0.1006, -0.1158, -0.1075, -0.1123,  0.0714, -0.1939]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5474, 0.4526, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3751, 0.3112, 0.3137, 0.0000, 0.0000, 0.0000],
         [0.2710, 0.2422, 0.2433, 0.2435, 0.0000, 0.0000],
         [0.2067, 0.1927, 0.1934, 0.1961, 0.2112, 0.0000],
         [0.1861, 0.1597, 0.1606, 0.1601, 0.1823, 0.1511]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5474, 0.4526, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3751, 0.3112, 0.3137, 0.0000, 0.0000, 0.0000],
         [0.2710, 0.2422, 0.2433, 0.2435, 0.0000, 0.0000],
         [0.2067, 0.1927, 0.1934, 0.1961, 0.2112, 0.0000],
         [0.1861, 0.1597, 0.1606, 0.1601, 0.1823, 0.1511]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6843, 0.5657, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4688, 0.3890, 0.3921, 0.0000, 0.0000, 0.0000],
         [0.3387, 0.3028, 0.3042, 0.0000, 0.0000, 0.0000],
         [0.2584, 0.0000, 0.2417, 0.0000, 0.2640, 0.0000],
         [0.0000, 0.1996, 0.2008, 0.0000, 0.2279, 0.0000]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6843, 0.5657, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3890, 0.3921, 0.0000, 0.0000, 0.0000],
         [0.3387, 0.3028, 0.0000, 0.3043, 0.0000, 0.0000],
         [0.2584, 0.2408, 0.2417, 0.0000, 0.2640, 0.0000],
         [0.2326, 0.0000, 0.0000, 0.2001, 0.2279, 0.1889]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-6.4502e-02, -1.1339e-01, -1.0889e-01, -7.3809e-02,  2.7147e-03,
          -1.2092e-01],
         [-3.4867e-02, -3.2208e-02, -3.5540e-02, -5.1690e-03, -8.5583e-02,
           2.5355e-02],
         [-3.5362e-02, -3.4223e-02, -3.7294e-02, -7.1027e-03, -8.2134e-02,
           2.0856e-02],
         [-1.0215e-02,  3.5928e-03,  3.6065e-05,  1.4044e-02, -6.4070e-02,
           4.8069e-02],
         [-3.4368e-02, -6.0992e-02, -5.8484e-02, -4.0017e-02,  3.1753e-03,
          -6.6229e-02],
         [-7.1622e-03,  2.0826e-02,  1.4706e-02,  3.1707e-02, -9.9715e-02,
           9.0805e-02]],

        [[-6.4502e-02, -1.1339e-01, -1.0889e-01, -7.3809e-02,  2.7147e-03,
          -1.2092e-01],
         [-3.4867e-02, -3.2208e-02, -3.5540e-02, -5.1690e-03, -8.5583e-02,
           2.5355e-02],
         [-3.5362e-02, -3.4223e-02, -3.7294e-02, -7.1027e-03, -8.2134e-02,
           2.0856e-02],
         [-1.0215e-02,  3.5928e-03,  3.6065e-05,  1.4044e-02, -6.4070e-02,
           4.8069e-02],
         [-3.4368e-02, -6.0992e-02, -5.8484e-02, -4.0017e-02,  3.1753e-03,
          -6.6229e-02],
         [-7.1622e-03,  2.0826e-02,  1.4706e-02,  3.1707e-02, -9.9715e-02,
           9.0805e-02]]], grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-6.4502e-02,        -inf,        -inf,        -inf,        -inf,
                 -inf],
         [-3.4867e-02, -3.2208e-02,        -inf,        -inf,        -inf,
                 -inf],
         [-3.5362e-02, -3.4223e-02, -3.7294e-02,        -inf,        -inf,
                 -inf],
         [-1.0215e-02,  3.5928e-03,  3.6065e-05,  1.4044e-02,        -inf,
                 -inf],
         [-3.4368e-02, -6.0992e-02, -5.8484e-02, -4.0017e-02,  3.1753e-03,
                 -inf],
         [-7.1622e-03,  2.0826e-02,  1.4706e-02,  3.1707e-02, -9.9715e-02,
           9.0805e-02]],

        [[-6.4502e-02,        -inf,        -inf,        -inf,        -inf,
                 -inf],
         [-3.4867e-02, -3.2208e-02,        -inf,        -inf,        -inf,
                 -inf],
         [-3.5362e-02, -3.4223e-02, -3.7294e-02,        -inf,        -inf,
                 -inf],
         [-1.0215e-02,  3.5928e-03,  3.6065e-05,  1.4044e-02,        -inf,
                 -inf],
         [-3.4368e-02, -6.0992e-02, -5.8484e-02, -4.0017e-02,  3.1753e-03,
                 -inf],
         [-7.1622e-03,  2.0826e-02,  1.4706e-02,  3.1707e-02, -9.9715e-02,
           9.0805e-02]]], grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4995, 0.5005, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3334, 0.3337, 0.3329, 0.0000, 0.0000, 0.0000],
         [0.2479, 0.2503, 0.2497, 0.2522, 0.0000, 0.0000],
         [0.2005, 0.1968, 0.1971, 0.1997, 0.2059, 0.0000],
         [0.1647, 0.1680, 0.1673, 0.1693, 0.1543, 0.1765]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4995, 0.5005, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3334, 0.3337, 0.3329, 0.0000, 0.0000, 0.0000],
         [0.2479, 0.2503, 0.2497, 0.2522, 0.0000, 0.0000],
         [0.2005, 0.1968, 0.1971, 0.1997, 0.2059, 0.0000],
         [0.1647, 0.1680, 0.1673, 0.1693, 0.1543, 0.1765]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6244, 0.6256, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4167, 0.4171, 0.4162, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3129, 0.3121, 0.3152, 0.0000, 0.0000],
         [0.2506, 0.2460, 0.2464, 0.2496, 0.2574, 0.0000],
         [0.2059, 0.2100, 0.2091, 0.2116, 0.1928, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6244, 0.6256, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4167, 0.4171, 0.4162, 0.0000, 0.0000, 0.0000],
         [0.3098, 0.0000, 0.0000, 0.3152, 0.0000, 0.0000],
         [0.2506, 0.2460, 0.2464, 0.2496, 0.2574, 0.0000],
         [0.2059, 0.2100, 0.2091, 0.0000, 0.1928, 0.2206]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.5668, -0.4678, -0.4679, -0.2035, -0.3373, -0.2229],
         [-0.6979, -0.7088, -0.7062, -0.3414, -0.4622, -0.3942],
         [-0.6874, -0.6961, -0.6937, -0.3349, -0.4545, -0.3865],
         [-0.3781, -0.4070, -0.4052, -0.2007, -0.2585, -0.2343],
         [-0.3055, -0.2724, -0.2720, -0.1236, -0.1889, -0.1384],
         [-0.5053, -0.5544, -0.5518, -0.2754, -0.3492, -0.3226]],

        [[-0.5668, -0.4678, -0.4679, -0.2035, -0.3373, -0.2229],
         [-0.6979, -0.7088, -0.7062, -0.3414, -0.4622, -0.3942],
         [-0.6874, -0.6961, -0.6937, -0.3349, -0.4545, -0.3865],
         [-0.3781, -0.4070, -0.4052, -0.2007, -0.2585, -0.2343],
         [-0.3055, -0.2724, -0.2720, -0.1236, -0.1889, -0.1384],
         [-0.5053, -0.5544, -0.5518, -0.2754, -0.3492, -0.3226]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.5668,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.6979, -0.7088,    -inf,    -inf,    -inf,    -inf],
         [-0.6874, -0.6961, -0.6937,    -inf,    -inf,    -inf],
         [-0.3781, -0.4070, -0.4052, -0.2007,    -inf,    -inf],
         [-0.3055, -0.2724, -0.2720, -0.1236, -0.1889,    -inf],
         [-0.5053, -0.5544, -0.5518, -0.2754, -0.3492, -0.3226]],

        [[-0.5668,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.6979, -0.7088,    -inf,    -inf,    -inf,    -inf],
         [-0.6874, -0.6961, -0.6937,    -inf,    -inf,    -inf],
         [-0.3781, -0.4070, -0.4052, -0.2007,    -inf,    -inf],
         [-0.3055, -0.2724, -0.2720, -0.1236, -0.1889,    -inf],
         [-0.5053, -0.5544, -0.5518, -0.2754, -0.3492, -0.3226]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5019, 0.4981, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3345, 0.3325, 0.3330, 0.0000, 0.0000, 0.0000],
         [0.2442, 0.2393, 0.2396, 0.2769, 0.0000, 0.0000],
         [0.1897, 0.1942, 0.1943, 0.2158, 0.2060, 0.0000],
         [0.1571, 0.1518, 0.1520, 0.1849, 0.1755, 0.1788]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5019, 0.4981, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3345, 0.3325, 0.3330, 0.0000, 0.0000, 0.0000],
         [0.2442, 0.2393, 0.2396, 0.2769, 0.0000, 0.0000],
         [0.1897, 0.1942, 0.1943, 0.2158, 0.2060, 0.0000],
         [0.1571, 0.1518, 0.1520, 0.1849, 0.1755, 0.1788]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6274, 0.6226, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4156, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3053, 0.2991, 0.0000, 0.3461, 0.0000, 0.0000],
         [0.0000, 0.2428, 0.2428, 0.2697, 0.2575, 0.0000],
         [0.1964, 0.0000, 0.1900, 0.2311, 0.2193, 0.2235]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6274, 0.6226, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3053, 0.0000, 0.0000, 0.3461, 0.0000, 0.0000],
         [0.0000, 0.2428, 0.2428, 0.2697, 0.2575, 0.0000],
         [0.1964, 0.1897, 0.1900, 0.2311, 0.2193, 0.2235]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-9.9663e-02,  3.4093e-02,  2.6771e-02,  5.8230e-02, -1.1281e-01,
           1.2842e-01],
         [ 1.3875e-01,  3.8445e-01,  3.7369e-01,  2.5021e-01,  7.4688e-02,
           3.6637e-01],
         [ 1.4473e-01,  3.8975e-01,  3.7908e-01,  2.5235e-01,  8.0053e-02,
           3.6794e-01],
         [ 8.6031e-02,  2.2105e-01,  2.1522e-01,  1.4185e-01,  4.9614e-02,
           2.0530e-01],
         [ 2.1150e-01,  3.7540e-01,  3.6923e-01,  2.1985e-01,  1.5402e-01,
           2.9262e-01],
         [ 3.2368e-02,  1.8278e-01,  1.7575e-01,  1.2977e-01, -3.3011e-04,
           2.0297e-01]],

        [[-9.9663e-02,  3.4093e-02,  2.6771e-02,  5.8230e-02, -1.1281e-01,
           1.2842e-01],
         [ 1.3875e-01,  3.8445e-01,  3.7369e-01,  2.5021e-01,  7.4688e-02,
           3.6637e-01],
         [ 1.4473e-01,  3.8975e-01,  3.7908e-01,  2.5235e-01,  8.0053e-02,
           3.6794e-01],
         [ 8.6031e-02,  2.2105e-01,  2.1522e-01,  1.4185e-01,  4.9614e-02,
           2.0530e-01],
         [ 2.1150e-01,  3.7540e-01,  3.6923e-01,  2.1985e-01,  1.5402e-01,
           2.9262e-01],
         [ 3.2368e-02,  1.8278e-01,  1.7575e-01,  1.2977e-01, -3.3011e-04,
           2.0297e-01]]], grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-9.9663e-02,        -inf,        -inf,        -inf,        -inf,
                 -inf],
         [ 1.3875e-01,  3.8445e-01,        -inf,        -inf,        -inf,
                 -inf],
         [ 1.4473e-01,  3.8975e-01,  3.7908e-01,        -inf,        -inf,
                 -inf],
         [ 8.6031e-02,  2.2105e-01,  2.1522e-01,  1.4185e-01,        -inf,
                 -inf],
         [ 2.1150e-01,  3.7540e-01,  3.6923e-01,  2.1985e-01,  1.5402e-01,
                 -inf],
         [ 3.2368e-02,  1.8278e-01,  1.7575e-01,  1.2977e-01, -3.3011e-04,
           2.0297e-01]],

        [[-9.9663e-02,        -inf,        -inf,        -inf,        -inf,
                 -inf],
         [ 1.3875e-01,  3.8445e-01,        -inf,        -inf,        -inf,
                 -inf],
         [ 1.4473e-01,  3.8975e-01,  3.7908e-01,        -inf,        -inf,
                 -inf],
         [ 8.6031e-02,  2.2105e-01,  2.1522e-01,  1.4185e-01,        -inf,
                 -inf],
         [ 2.1150e-01,  3.7540e-01,  3.6923e-01,  2.1985e-01,  1.5402e-01,
                 -inf],
         [ 3.2368e-02,  1.8278e-01,  1.7575e-01,  1.2977e-01, -3.3011e-04,
           2.0297e-01]]], grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4567, 0.5433, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2968, 0.3529, 0.3503, 0.0000, 0.0000, 0.0000],
         [0.2361, 0.2597, 0.2586, 0.2456, 0.0000, 0.0000],
         [0.1921, 0.2156, 0.2147, 0.1932, 0.1844, 0.0000],
         [0.1564, 0.1739, 0.1730, 0.1675, 0.1528, 0.1764]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4567, 0.5433, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.2968, 0.3529, 0.3503, 0.0000, 0.0000, 0.0000],
         [0.2361, 0.2597, 0.2586, 0.2456, 0.0000, 0.0000],
         [0.1921, 0.2156, 0.2147, 0.1932, 0.1844, 0.0000],
         [0.1564, 0.1739, 0.1730, 0.1675, 0.1528, 0.1764]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.6792, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3710, 0.4412, 0.4378, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.3233, 0.3070, 0.0000, 0.0000],
         [0.2401, 0.0000, 0.2684, 0.2415, 0.2305, 0.0000],
         [0.1954, 0.2174, 0.2163, 0.2094, 0.1910, 0.2205]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5708, 0.6792, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3710, 0.4412, 0.4378, 0.0000, 0.0000, 0.0000],
         [0.2951, 0.0000, 0.3233, 0.3070, 0.0000, 0.0000],
         [0.2401, 0.2696, 0.2684, 0.2415, 0.2305, 0.0000],
         [0.1954, 0.2174, 0.2163, 0.2094, 0.1910, 0.2205]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.0214, -0.0285, -0.0317, -0.0055, -0.0796,  0.0236],
         [ 0.0484, -0.0115, -0.0158, -0.0024, -0.0902,  0.0395],
         [ 0.0585, -0.0067, -0.0109, -0.0015, -0.0835,  0.0390],
         [ 0.0051, -0.0163, -0.0191, -0.0032, -0.0632,  0.0226],
         [ 0.2236,  0.0814,  0.0816,  0.0151,  0.0613,  0.0184],
         [-0.0900, -0.0627, -0.0673, -0.0119, -0.1311,  0.0292]],

        [[-0.0214, -0.0285, -0.0317, -0.0055, -0.0796,  0.0236],
         [ 0.0484, -0.0115, -0.0158, -0.0024, -0.0902,  0.0395],
         [ 0.0585, -0.0067, -0.0109, -0.0015, -0.0835,  0.0390],
         [ 0.0051, -0.0163, -0.0191, -0.0032, -0.0632,  0.0226],
         [ 0.2236,  0.0814,  0.0816,  0.0151,  0.0613,  0.0184],
         [-0.0900, -0.0627, -0.0673, -0.0119, -0.1311,  0.0292]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.0214,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0484, -0.0115,    -inf,    -inf,    -inf,    -inf],
         [ 0.0585, -0.0067, -0.0109,    -inf,    -inf,    -inf],
         [ 0.0051, -0.0163, -0.0191, -0.0032,    -inf,    -inf],
         [ 0.2236,  0.0814,  0.0816,  0.0151,  0.0613,    -inf],
         [-0.0900, -0.0627, -0.0673, -0.0119, -0.1311,  0.0292]],

        [[-0.0214,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.0484, -0.0115,    -inf,    -inf,    -inf,    -inf],
         [ 0.0585, -0.0067, -0.0109,    -inf,    -inf,    -inf],
         [ 0.0051, -0.0163, -0.0191, -0.0032,    -inf,    -inf],
         [ 0.2236,  0.0814,  0.0816,  0.0151,  0.0613,    -inf],
         [-0.0900, -0.0627, -0.0673, -0.0119, -0.1311,  0.0292]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5106, 0.4894, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3440, 0.3285, 0.3275, 0.0000, 0.0000, 0.0000],
         [0.2524, 0.2486, 0.2481, 0.2509, 0.0000, 0.0000],
         [0.2191, 0.1982, 0.1982, 0.1891, 0.1954, 0.0000],
         [0.1626, 0.1657, 0.1652, 0.1718, 0.1579, 0.1768]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5106, 0.4894, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3440, 0.3285, 0.3275, 0.0000, 0.0000, 0.0000],
         [0.2524, 0.2486, 0.2481, 0.2509, 0.0000, 0.0000],
         [0.2191, 0.1982, 0.1982, 0.1891, 0.1954, 0.0000],
         [0.1626, 0.1657, 0.1652, 0.1718, 0.1579, 0.1768]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6382, 0.6118, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4106, 0.4094, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3107, 0.3101, 0.3136, 0.0000, 0.0000],
         [0.2739, 0.0000, 0.2477, 0.0000, 0.2442, 0.0000],
         [0.2032, 0.2072, 0.2065, 0.2147, 0.1974, 0.2211]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6382, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4300, 0.4106, 0.4094, 0.0000, 0.0000, 0.0000],
         [0.3155, 0.3107, 0.3101, 0.3136, 0.0000, 0.0000],
         [0.2739, 0.0000, 0.0000, 0.2364, 0.2442, 0.0000],
         [0.2032, 0.2072, 0.2065, 0.0000, 0.0000, 0.0000]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[ 0.2376, -0.1203, -0.1113, -0.1392,  0.0825, -0.2269],
         [ 0.1111,  0.0118,  0.0155, -0.0209,  0.0783, -0.0551],
         [ 0.1150,  0.0091,  0.0130, -0.0237,  0.0793, -0.0594],
         [ 0.0209,  0.0342,  0.0347,  0.0168,  0.0334,  0.0136],
         [ 0.1527, -0.0416, -0.0361, -0.0663,  0.0738, -0.1191],
         [-0.0106,  0.0590,  0.0582,  0.0410,  0.0277,  0.0503]],

        [[ 0.2376, -0.1203, -0.1113, -0.1392,  0.0825, -0.2269],
         [ 0.1111,  0.0118,  0.0155, -0.0209,  0.0783, -0.0551],
         [ 0.1150,  0.0091,  0.0130, -0.0237,  0.0793, -0.0594],
         [ 0.0209,  0.0342,  0.0347,  0.0168,  0.0334,  0.0136],
         [ 0.1527, -0.0416, -0.0361, -0.0663,  0.0738, -0.1191],
         [-0.0106,  0.0590,  0.0582,  0.0410,  0.0277,  0.0503]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[ 0.2376,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.1111,  0.0118,    -inf,    -inf,    -inf,    -inf],
         [ 0.1150,  0.0091,  0.0130,    -inf,    -inf,    -inf],
         [ 0.0209,  0.0342,  0.0347,  0.0168,    -inf,    -inf],
         [ 0.1527, -0.0416, -0.0361, -0.0663,  0.0738,    -inf],
         [-0.0106,  0.0590,  0.0582,  0.0410,  0.0277,  0.0503]],

        [[ 0.2376,    -inf,    -inf,    -inf,    -inf,    -inf],
         [ 0.1111,  0.0118,    -inf,    -inf,    -inf,    -inf],
         [ 0.1150,  0.0091,  0.0130,    -inf,    -inf,    -inf],
         [ 0.0209,  0.0342,  0.0347,  0.0168,    -inf,    -inf],
         [ 0.1527, -0.0416, -0.0361, -0.0663,  0.0738,    -inf],
         [-0.0106,  0.0590,  0.0582,  0.0410,  0.0277,  0.0503]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5175, 0.4825, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3499, 0.3246, 0.3255, 0.0000, 0.0000, 0.0000],
         [0.2490, 0.2513, 0.2514, 0.2483, 0.0000, 0.0000],
         [0.2198, 0.1916, 0.1924, 0.1883, 0.2079, 0.0000],
         [0.1611, 0.1692, 0.1691, 0.1670, 0.1655, 0.1681]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5175, 0.4825, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3499, 0.3246, 0.3255, 0.0000, 0.0000, 0.0000],
         [0.2490, 0.2513, 0.2514, 0.2483, 0.0000, 0.0000],
         [0.2198, 0.1916, 0.1924, 0.1883, 0.2079, 0.0000],
         [0.1611, 0.1692, 0.1691, 0.1670, 0.1655, 0.1681]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.6031, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.4058, 0.4069, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.3143, 0.0000, 0.0000, 0.0000],
         [0.2748, 0.2395, 0.2404, 0.2354, 0.2599, 0.0000],
         [0.2013, 0.2115, 0.2114, 0.2088, 0.0000, 0.2102]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6469, 0.6031, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4373, 0.4058, 0.4069, 0.0000, 0.0000, 0.0000],
         [0.3112, 0.3142, 0.3143, 0.3103, 0.0000, 0.0000],
         [0.2748, 0.2395, 0.2404, 0.2354, 0.2599, 0.0000],
         [0.2013, 0.2115, 0.2114, 0.2088, 0.2068, 0.0000]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.5228, -0.6706, -0.6653, -0.3533, -0.3824, -0.4299],
         [-0.6302, -0.7772, -0.7719, -0.4031, -0.4592, -0.4840],
         [-0.6254, -0.7712, -0.7659, -0.4000, -0.4558, -0.4801],
         [-0.3262, -0.3988, -0.3962, -0.2061, -0.2375, -0.2467],
         [-0.3642, -0.4455, -0.4425, -0.2303, -0.2652, -0.2757],
         [-0.3961, -0.4870, -0.4838, -0.2523, -0.2885, -0.3026]],

        [[-0.5228, -0.6706, -0.6653, -0.3533, -0.3824, -0.4299],
         [-0.6302, -0.7772, -0.7719, -0.4031, -0.4592, -0.4840],
         [-0.6254, -0.7712, -0.7659, -0.4000, -0.4558, -0.4801],
         [-0.3262, -0.3988, -0.3962, -0.2061, -0.2375, -0.2467],
         [-0.3642, -0.4455, -0.4425, -0.2303, -0.2652, -0.2757],
         [-0.3961, -0.4870, -0.4838, -0.2523, -0.2885, -0.3026]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.5228,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.6302, -0.7772,    -inf,    -inf,    -inf,    -inf],
         [-0.6254, -0.7712, -0.7659,    -inf,    -inf,    -inf],
         [-0.3262, -0.3988, -0.3962, -0.2061,    -inf,    -inf],
         [-0.3642, -0.4455, -0.4425, -0.2303, -0.2652,    -inf],
         [-0.3961, -0.4870, -0.4838, -0.2523, -0.2885, -0.3026]],

        [[-0.5228,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.6302, -0.7772,    -inf,    -inf,    -inf,    -inf],
         [-0.6254, -0.7712, -0.7659,    -inf,    -inf,    -inf],
         [-0.3262, -0.3988, -0.3962, -0.2061,    -inf,    -inf],
         [-0.3642, -0.4455, -0.4425, -0.2303, -0.2652,    -inf],
         [-0.3961, -0.4870, -0.4838, -0.2523, -0.2885, -0.3026]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5260, 0.4740, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3562, 0.3213, 0.3225, 0.0000, 0.0000, 0.0000],
         [0.2506, 0.2381, 0.2385, 0.2728, 0.0000, 0.0000],
         [0.1975, 0.1865, 0.1869, 0.2172, 0.2119, 0.0000],
         [0.1631, 0.1529, 0.1533, 0.1805, 0.1760, 0.1742]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5260, 0.4740, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3562, 0.3213, 0.3225, 0.0000, 0.0000, 0.0000],
         [0.2506, 0.2381, 0.2385, 0.2728, 0.0000, 0.0000],
         [0.1975, 0.1865, 0.1869, 0.2172, 0.2119, 0.0000],
         [0.1631, 0.1529, 0.1533, 0.1805, 0.1760, 0.1742]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.6575, 0.5925, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4452, 0.4016, 0.4031, 0.0000, 0.0000, 0.0000],
         [0.3133, 0.0000, 0.2981, 0.0000, 0.0000, 0.0000],
         [0.2469, 0.0000, 0.2336, 0.2715, 0.2648, 0.0000],
         [0.2039, 0.1912, 0.1916, 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.5925, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.4031, 0.0000, 0.0000, 0.0000],
         [0.3133, 0.2976, 0.0000, 0.3410, 0.0000, 0.0000],
         [0.2469, 0.0000, 0.2336, 0.2715, 0.0000, 0.0000],
         [0.2039, 0.1912, 0.0000, 0.2257, 0.2200, 0.0000]]],
       grad_fn=<MulBackward0>)

class SelfAttentionV2Causal: attn_scores pre Mask:
 tensor([[[-0.1242, -0.0270, -0.0294,  0.0095, -0.0635,  0.0300],
         [-0.1563, -0.0407, -0.0430,  0.0057, -0.0712,  0.0243],
         [-0.1552, -0.0433, -0.0452,  0.0030, -0.0669,  0.0184],
         [-0.0817, -0.0139, -0.0158,  0.0099, -0.0469,  0.0275],
         [-0.0910, -0.0779, -0.0734, -0.0474,  0.0292, -0.0942],
         [-0.0986,  0.0094,  0.0042,  0.0365, -0.0907,  0.0856]],

        [[-0.1242, -0.0270, -0.0294,  0.0095, -0.0635,  0.0300],
         [-0.1563, -0.0407, -0.0430,  0.0057, -0.0712,  0.0243],
         [-0.1552, -0.0433, -0.0452,  0.0030, -0.0669,  0.0184],
         [-0.0817, -0.0139, -0.0158,  0.0099, -0.0469,  0.0275],
         [-0.0910, -0.0779, -0.0734, -0.0474,  0.0292, -0.0942],
         [-0.0986,  0.0094,  0.0042,  0.0365, -0.0907,  0.0856]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.1242,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1563, -0.0407,    -inf,    -inf,    -inf,    -inf],
         [-0.1552, -0.0433, -0.0452,    -inf,    -inf,    -inf],
         [-0.0817, -0.0139, -0.0158,  0.0099,    -inf,    -inf],
         [-0.0910, -0.0779, -0.0734, -0.0474,  0.0292,    -inf],
         [-0.0986,  0.0094,  0.0042,  0.0365, -0.0907,  0.0856]],

        [[-0.1242,    -inf,    -inf,    -inf,    -inf,    -inf],
         [-0.1563, -0.0407,    -inf,    -inf,    -inf,    -inf],
         [-0.1552, -0.0433, -0.0452,    -inf,    -inf,    -inf],
         [-0.0817, -0.0139, -0.0158,  0.0099,    -inf,    -inf],
         [-0.0910, -0.0779, -0.0734, -0.0474,  0.0292,    -inf],
         [-0.0986,  0.0094,  0.0042,  0.0365, -0.0907,  0.0856]]],
       grad_fn=<MaskedFillBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4796, 0.5204, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3161, 0.3422, 0.3417, 0.0000, 0.0000, 0.0000],
         [0.2402, 0.2520, 0.2516, 0.2562, 0.0000, 0.0000],
         [0.1945, 0.1963, 0.1969, 0.2006, 0.2117, 0.0000],
         [0.1563, 0.1687, 0.1680, 0.1719, 0.1571, 0.1780]],

        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.4796, 0.5204, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3161, 0.3422, 0.3417, 0.0000, 0.0000, 0.0000],
         [0.2402, 0.2520, 0.2516, 0.2562, 0.0000, 0.0000],
         [0.1945, 0.1963, 0.1969, 0.2006, 0.2117, 0.0000],
         [0.1563, 0.1687, 0.1680, 0.1719, 0.1571, 0.1780]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5995, 0.6505, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3952, 0.4277, 0.4271, 0.0000, 0.0000, 0.0000],
         [0.3002, 0.3150, 0.0000, 0.3203, 0.0000, 0.0000],
         [0.2431, 0.2454, 0.2462, 0.2507, 0.2647, 0.0000],
         [0.1953, 0.2108, 0.2101, 0.0000, 0.1964, 0.2225]],

        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.5995, 0.6505, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.3952, 0.4277, 0.4271, 0.0000, 0.0000, 0.0000],
         [0.3002, 0.3150, 0.3145, 0.3203, 0.0000, 0.0000],
         [0.2431, 0.2454, 0.0000, 0.0000, 0.2647, 0.0000],
         [0.1953, 0.2108, 0.2101, 0.2149, 0.1964, 0.2225]]],
       grad_fn=<MulBackward0>)
context vecs: 
 tensor([[[-5.6490e-01,  2.7701e-01,  5.9649e-01,  1.3293e-01,  0.0000e+00,
           0.0000e+00, -7.1045e-01,  6.3294e-01, -6.0269e-01,  5.4196e-01,
           0.0000e+00,  0.0000e+00,  5.8435e-01, -3.4911e-01,  6.4864e-04,
           5.5836e-01, -7.5665e-01,  8.2183e-01, -8.8929e-01,  9.5343e-02,
          -2.9380e-01, -5.2863e-01,  1.6703e-01, -3.0837e-01,  1.0248e-03,
           4.5908e-02,  0.0000e+00,  0.0000e+00,  4.4402e-01,  4.0996e-02,
           2.0191e-01,  1.0548e-01,  5.8828e-01,  2.5693e-01, -2.9860e-01,
           3.3874e-01,  3.3022e-01,  4.8965e-01,  0.0000e+00,  0.0000e+00,
          -5.8344e-02,  1.6706e-01, -1.6548e-01, -3.5258e-01,  8.7113e-01,
           4.4766e-01,  9.3155e-02,  8.2223e-02,  0.0000e+00,  0.0000e+00],
         [-2.7300e-01,  1.3387e-01,  7.3634e-01,  4.0708e-01,  7.2400e-01,
           3.7634e-01, -6.7347e-01,  8.0585e-01, -6.7104e-01,  6.8537e-01,
           0.0000e+00,  0.0000e+00,  2.8588e-01, -1.7080e-01, -7.1532e-02,
           5.3967e-01, -6.8785e-01,  8.2053e-01, -9.9265e-01,  3.0104e-01,
           2.1561e-02, -4.0150e-01,  1.2067e-02, -9.2825e-02,  2.7389e-02,
          -1.9827e-01,  7.3260e-03,  1.1710e-01,  5.9328e-01,  1.8052e-01,
           1.2245e-01,  2.3782e-01,  6.1348e-01,  1.6350e-01, -2.9905e-01,
           4.4899e-01,  7.0805e-02,  4.1401e-01, -9.2386e-02,  3.4872e-01,
           2.3642e-01,  1.0218e-01, -1.3476e-01, -2.2719e-01,  5.9653e-01,
           6.4948e-02,  3.6406e-01, -6.9583e-02, -6.1157e-03, -4.2861e-01],
         [-6.0732e-01, -1.6734e-01,  4.7173e-01,  2.6046e-01,  5.6837e-01,
           2.6055e-01, -6.5522e-01,  8.6919e-01, -4.8083e-01,  5.4485e-01,
          -2.8941e-01,  2.5252e-01,  5.7227e-01,  1.1183e-02,  0.0000e+00,
           0.0000e+00, -6.6416e-01,  8.1719e-01, -1.0205e+00,  3.5794e-01,
           1.3916e-02, -2.6091e-01,  6.4358e-02, -1.6599e-01,  3.8655e-02,
          -2.7283e-01, -7.0027e-02,  1.9397e-01,  0.0000e+00,  0.0000e+00,
           8.8130e-03,  1.2390e-01,  1.9397e-01,  8.4715e-02, -2.9305e-01,
           4.8308e-01, -1.9902e-02,  3.8852e-01,  3.9364e-02,  1.4876e-01,
           2.9251e-01,  1.7910e-01, -6.1533e-02, -6.2524e-02,  8.0088e-01,
           8.8076e-02,  4.6066e-01, -1.1797e-01, -7.7708e-02, -5.1494e-01],
         [-3.6460e-01,  5.0830e-03,  4.6644e-01,  2.8810e-01,  5.6060e-01,
           2.5806e-01, -2.7372e-01,  3.1718e-01, -6.1714e-01,  6.6392e-01,
          -2.6309e-01,  3.1687e-01,  4.2777e-01,  6.4520e-03, -2.7679e-02,
           2.1966e-01, -3.7716e-01,  5.0880e-01, -7.6661e-01,  2.6825e-01,
           2.0127e-01, -1.7908e-01,  5.5223e-02, -1.8312e-01,  1.3675e-02,
          -9.8624e-02, -3.2328e-02,  1.1617e-01,  3.9949e-01,  1.4364e-01,
           7.3690e-02,  2.2461e-01,  5.5253e-01,  9.9561e-02, -2.2160e-01,
           3.6843e-01, -1.3066e-01,  2.0374e-01, -8.2262e-03,  2.2897e-01,
           1.7716e-01,  7.4578e-02, -6.8946e-02, -4.9971e-02,  3.0856e-01,
           3.4274e-02,  1.8156e-01, -3.1735e-02, -5.1532e-02, -3.0843e-01],
         [-4.5939e-01, -1.3884e-01,  4.9388e-01,  3.0254e-01,  5.9800e-01,
           2.7060e-01, -2.2421e-01,  4.0306e-01, -4.3174e-01,  4.8594e-01,
          -1.9266e-01,  2.0233e-01,  2.1716e-01, -5.1270e-02, -5.0900e-02,
           1.4977e-01, -4.5109e-01,  5.6872e-01, -6.4872e-01,  2.5580e-01,
           1.6154e-01, -2.9656e-01,  1.4467e-02, -2.0008e-01,  6.4305e-02,
          -2.5815e-01, -1.1173e-01,  1.8136e-01,  5.5946e-01,  1.9227e-01,
           1.2738e-01,  2.3974e-01,  4.6133e-01,  1.0362e-01, -8.3590e-02,
           1.6676e-01, -1.1782e-01,  3.0459e-01,  7.6099e-02,  3.1879e-01,
           2.2854e-01,  9.0762e-02, -2.2111e-03, -1.0167e-01,  9.3859e-01,
           1.7881e-01,  3.2658e-01, -3.4087e-02, -8.7512e-02, -4.9562e-01],
         [-5.8362e-01, -1.0290e-01,  4.8931e-01,  3.3044e-01,  5.6671e-01,
           2.5239e-01, -3.9749e-01,  5.7851e-01, -3.6622e-01,  3.8270e-01,
          -1.0809e-01,  1.5499e-01,  3.4956e-01,  6.6894e-02, -9.3077e-02,
           3.8925e-01, -2.7305e-01,  3.4762e-01, -6.3660e-01,  2.4958e-01,
           1.6210e-01, -2.6306e-01,  2.9477e-02, -1.6737e-01,  4.1754e-02,
          -2.7743e-01, -9.1257e-02,  1.6491e-01,  4.5659e-01,  1.5415e-01,
           7.2281e-02,  2.8503e-01,  3.7481e-01,  6.6162e-02, -6.4970e-02,
           1.7301e-01, -9.3900e-02,  2.5157e-01,  2.7442e-02,  2.5985e-01,
           2.9317e-01,  1.4196e-01, -7.8894e-02, -1.0187e-01,  8.0232e-01,
           1.1939e-01,  2.1850e-01, -5.6625e-02, -9.0522e-02, -4.0854e-01]],

        [[-5.6490e-01,  2.7701e-01,  5.9649e-01,  1.3293e-01,  0.0000e+00,
           0.0000e+00, -7.1045e-01,  6.3294e-01, -6.0269e-01,  5.4196e-01,
          -1.8391e-01,  5.1321e-01,  5.8435e-01, -3.4911e-01,  6.4864e-04,
           5.5836e-01, -7.5665e-01,  8.2183e-01, -8.8929e-01,  9.5343e-02,
          -2.9380e-01, -5.2863e-01,  1.6703e-01, -3.0837e-01,  1.0248e-03,
           4.5908e-02,  0.0000e+00,  0.0000e+00,  4.4402e-01,  4.0996e-02,
           0.0000e+00,  0.0000e+00,  5.8828e-01,  2.5693e-01, -2.9860e-01,
           3.3874e-01,  0.0000e+00,  0.0000e+00, -3.0156e-01,  2.5075e-01,
          -5.8344e-02,  1.6706e-01, -1.6548e-01, -3.5258e-01,  8.7113e-01,
           4.4766e-01,  0.0000e+00,  0.0000e+00,  2.2941e-01, -1.5824e-01],
         [-7.3429e-01,  7.2201e-03,  4.7389e-01,  3.4859e-01,  7.2400e-01,
           3.7634e-01, -3.9959e-01,  3.5600e-01, -6.7104e-01,  6.8537e-01,
          -3.1167e-01,  4.4352e-01,  5.8139e-01, -7.3702e-02, -7.1868e-02,
           2.5008e-01, -3.7673e-01,  4.0918e-01, -9.9265e-01,  3.0104e-01,
           2.1561e-02, -4.0150e-01,  9.6205e-02, -2.4816e-01,  2.7389e-02,
          -1.9827e-01,  7.3260e-03,  1.1710e-01,  5.9328e-01,  1.8052e-01,
           1.2245e-01,  2.3782e-01,  6.1348e-01,  1.6350e-01, -2.9905e-01,
           4.4899e-01,  7.0805e-02,  4.1401e-01, -9.2386e-02,  3.4872e-01,
           2.0978e-01,  1.7847e-01, -8.4490e-02, -1.8002e-01,  1.0474e+00,
           2.9663e-01,  3.1506e-01, -1.1283e-01, -6.1157e-03, -4.2861e-01],
         [-4.8328e-01,  4.5520e-03,  4.7230e-01,  2.5961e-01,  7.8110e-01,
           3.8770e-01, -4.6441e-01,  5.5577e-01, -6.9318e-01,  7.3581e-01,
          -3.4776e-01,  4.1536e-01,  5.7227e-01,  1.1183e-02, -4.6517e-02,
           3.5693e-01, -6.6416e-01,  8.1719e-01, -1.0205e+00,  3.5794e-01,
           1.4780e-02, -2.6064e-01,  6.9574e-02, -2.3116e-01,  3.8332e-02,
          -2.8729e-01, -7.0027e-02,  1.9397e-01,  6.4225e-01,  2.2462e-01,
           2.4923e-02,  2.5192e-01,  6.1547e-01,  1.3402e-01, -1.8105e-01,
           3.5603e-01, -1.9902e-02,  3.8852e-01,  0.0000e+00,  0.0000e+00,
           2.9251e-01,  1.7910e-01, -1.1845e-01, -1.8381e-01,  1.1056e+00,
           2.4469e-01,  2.1393e-01, -7.0773e-02, -7.7708e-02, -5.1494e-01],
         [-7.0932e-01, -1.0532e-01,  3.5341e-01,  1.8982e-01,  3.7095e-01,
           1.9040e-01, -4.9682e-01,  6.6831e-01, -6.1714e-01,  6.6392e-01,
          -1.0710e-01,  9.2580e-02,  3.7438e-01,  1.5400e-02, -9.9544e-02,
           4.6906e-01, -5.6245e-01,  7.1006e-01, -9.0984e-01,  3.5511e-01,
           1.3011e-01, -3.0712e-01,  5.5223e-02, -1.8312e-01,  2.9394e-02,
          -2.0714e-01, -8.4352e-02,  2.0084e-01,  2.9670e-01,  8.9464e-02,
           7.3690e-02,  2.2461e-01,  3.9598e-01,  8.0575e-02, -2.0139e-01,
           3.2627e-01,  4.8689e-02,  1.5615e-01, -3.6560e-02,  1.2190e-01,
           1.6338e-01,  1.1401e-01, -1.1071e-01, -1.3896e-01,  1.0012e+00,
           1.8164e-01,  2.9251e-01, -9.4919e-02, -1.0593e-01, -4.8002e-01],
         [-5.0733e-01, -7.1876e-02,  4.9383e-01,  3.0322e-01,  4.3796e-01,
           2.0161e-01, -3.8013e-01,  5.4197e-01, -5.7366e-01,  6.4618e-01,
          -2.6019e-01,  2.8624e-01,  4.0345e-01,  4.8850e-02, -2.2502e-02,
           1.7495e-01, -2.0633e-01,  2.4617e-01, -1.0500e-01, -4.8190e-03,
           1.5582e-01, -1.2725e-01,  1.4467e-02, -2.0008e-01,  6.4305e-02,
          -2.5815e-01, -1.3356e-01,  1.8761e-01,  4.6920e-01,  1.8252e-01,
           5.8150e-02,  1.8200e-01,  3.4002e-01,  8.8905e-02, -1.4131e-01,
           2.7895e-01, -1.1782e-01,  3.0459e-01,  7.6099e-02,  3.1879e-01,
           3.2238e-01,  1.3132e-01, -2.2418e-03, -8.4975e-02,  9.3859e-01,
           1.7881e-01,  2.3069e-01, -7.1625e-02, -5.4864e-03, -2.8334e-01],
         [-6.6238e-01, -1.3508e-01,  6.3457e-01,  4.3669e-01,  5.5677e-01,
           2.7899e-01, -4.9967e-01,  7.4634e-01, -5.5988e-01,  6.2138e-01,
          -2.5997e-01,  2.2680e-01,  3.8010e-01,  4.4436e-02, -5.0551e-02,
           2.5488e-01, -3.9287e-01,  4.9662e-01, -4.7660e-01,  2.2670e-01,
           3.6768e-02, -1.5714e-01,  2.9477e-02, -1.6737e-01,  4.1754e-02,
          -2.7743e-01, -1.0948e-01,  1.4583e-01,  4.6919e-01,  2.0279e-01,
           7.2281e-02,  2.8503e-01,  4.8467e-01,  7.8556e-02, -1.1919e-01,
           2.1248e-01, -7.2100e-02,  2.5714e-01,  4.5410e-02,  3.2775e-01,
           2.9317e-01,  1.4196e-01, -5.7939e-02, -8.8856e-02,  7.8659e-01,
           1.4066e-01,  2.6988e-01, -3.0761e-02, -1.2434e-01, -4.7540e-01]]],
       grad_fn=<CatBackward0>)
context vecs shape:  torch.Size([2, 6, 50])
