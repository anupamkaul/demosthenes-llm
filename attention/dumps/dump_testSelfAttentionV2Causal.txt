python test_SelfAttentionV2Causal.py
torch.Size([2, 6, 3])
tensor([[[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]],

        [[0.4300, 0.1500, 0.8900],
         [0.5500, 0.8700, 0.6600],
         [0.5700, 0.8500, 0.6400],
         [0.2200, 0.5800, 0.3300],
         [0.7700, 0.2500, 0.1000],
         [0.0500, 0.8000, 0.5500]]])

class SelfAttentionV2Causal: attn_scores post Mask:
 tensor([[[-0.4028, -0.2063, -0.2069, -0.0635, -0.1611, -0.0672],
         [-0.2623,  0.1610,  0.1602,  0.1450,  0.1019,  0.1546],
         [-0.2630,  0.1553,  0.1546,  0.1416,  0.0979,  0.1510],
         [-0.0989,  0.1501,  0.1497,  0.1111,  0.1010,  0.1183],
         [-0.2004,  0.0102,  0.0098,  0.0397, -0.0013,  0.0425],
         [-0.1048,  0.2070,  0.2065,  0.1480,  0.1407,  0.1575]],

        [[-0.4028, -0.2063, -0.2069, -0.0635, -0.1611, -0.0672],
         [-0.2623,  0.1610,  0.1602,  0.1450,  0.1019,  0.1546],
         [-0.2630,  0.1553,  0.1546,  0.1416,  0.0979,  0.1510],
         [-0.0989,  0.1501,  0.1497,  0.1111,  0.1010,  0.1183],
         [-0.2004,  0.0102,  0.0098,  0.0397, -0.0013,  0.0425],
         [-0.1048,  0.2070,  0.2065,  0.1480,  0.1407,  0.1575]]],
       grad_fn=<UnsafeViewBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot:
 tensor([[[0.1466, 0.1642, 0.1642, 0.1784, 0.1686, 0.1780],
         [0.1365, 0.1743, 0.1743, 0.1727, 0.1685, 0.1737],
         [0.1368, 0.1742, 0.1741, 0.1728, 0.1685, 0.1737],
         [0.1494, 0.1725, 0.1725, 0.1686, 0.1677, 0.1694],
         [0.1497, 0.1691, 0.1690, 0.1720, 0.1680, 0.1723],
         [0.1456, 0.1743, 0.1743, 0.1685, 0.1678, 0.1694]],

        [[0.1466, 0.1642, 0.1642, 0.1784, 0.1686, 0.1780],
         [0.1365, 0.1743, 0.1743, 0.1727, 0.1685, 0.1737],
         [0.1368, 0.1742, 0.1741, 0.1728, 0.1685, 0.1737],
         [0.1494, 0.1725, 0.1725, 0.1686, 0.1677, 0.1694],
         [0.1497, 0.1691, 0.1690, 0.1720, 0.1680, 0.1723],
         [0.1456, 0.1743, 0.1743, 0.1685, 0.1678, 0.1694]]],
       grad_fn=<SoftmaxBackward0>)

class SelfAttentionV2Causal: att_wts post scaled dot and dropout:
 tensor([[[0.1466, 0.1642, 0.1642, 0.1784, 0.1686, 0.1780],
         [0.1365, 0.1743, 0.1743, 0.1727, 0.1685, 0.1737],
         [0.1368, 0.1742, 0.1741, 0.1728, 0.1685, 0.1737],
         [0.1494, 0.1725, 0.1725, 0.1686, 0.1677, 0.1694],
         [0.1497, 0.1691, 0.1690, 0.1720, 0.1680, 0.1723],
         [0.1456, 0.1743, 0.1743, 0.1685, 0.1678, 0.1694]],

        [[0.1466, 0.1642, 0.1642, 0.1784, 0.1686, 0.1780],
         [0.1365, 0.1743, 0.1743, 0.1727, 0.1685, 0.1737],
         [0.1368, 0.1742, 0.1741, 0.1728, 0.1685, 0.1737],
         [0.1494, 0.1725, 0.1725, 0.1686, 0.1677, 0.1694],
         [0.1497, 0.1691, 0.1690, 0.1720, 0.1680, 0.1723],
         [0.1456, 0.1743, 0.1743, 0.1685, 0.1678, 0.1694]]],
       grad_fn=<SoftmaxBackward0>)
context_vecs.shape: torch.Size([2, 6, 3])
context vecs:
 tensor([[[ 0.2633,  0.4277, -0.1353],
         [ 0.2641,  0.4296, -0.1350],
         [ 0.2641,  0.4296, -0.1350],
         [ 0.2647,  0.4316, -0.1381],
         [ 0.2642,  0.4303, -0.1373],
         [ 0.2648,  0.4316, -0.1375]],

        [[ 0.2633,  0.4277, -0.1353],
         [ 0.2641,  0.4296, -0.1350],
         [ 0.2641,  0.4296, -0.1350],
         [ 0.2647,  0.4316, -0.1381],
         [ 0.2642,  0.4303, -0.1373],
         [ 0.2648,  0.4316, -0.1375]]], grad_fn=<UnsafeViewBackward0>)
