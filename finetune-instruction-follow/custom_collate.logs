Len of data:  1100
Training set length: 935
Test set length: 110
Validation set length: 55
[50256]
custom collate draft 1 (batch-1):
 tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
custom collate draft 1 (batch-2):
 tensor([[    1,     2,     3,     4,     5,     6,     7,     7,    98,    99,
           102,   103,   999],
        [    1,     3, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256],
        [    0,     1,     2,     3,     4,     5,     6, 50256, 50256, 50256,
         50256, 50256, 50256]])


# see here that there are 2 tensors now, the target tensor (second) is left shifted by
# 1, as that is the ideal output sequence since the LLM predicts one token at a time

custom collate draft 2 (batch-1):
 (tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]]), tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256, 50256, 50256, 50256],
        [    8,     9, 50256, 50256, 50256]]))
custom collate draft 2 (batch-2):
 (tensor([[    1,     2,     3,     4,     5,     6,     7,     7,    98,    99,
           102,   103,   999],
        [    1,     3, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256],
        [    0,     1,     2,     3,     4,     5,     6, 50256, 50256, 50256,
         50256, 50256, 50256]]), tensor([[    2,     3,     4,     5,     6,     7,     7,    98,    99,   102,
           103,   999, 50256],
        [    3, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256],
        [    1,     2,     3,     4,     5,     6, 50256, 50256, 50256, 50256,
         50256, 50256, 50256]]))
