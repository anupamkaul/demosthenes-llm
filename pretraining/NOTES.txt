Weight parameters

In the context of LLMs and other deep learning models, weights refer to the trainable parameters that the learning process adjusts.
These weights are also known as weight parameters or simply parameters. In frameworks like PyTorch, these weights are stored in 
linear layers; we used these to implement the multi-head attention module in "attention" and the GPTModel in "llm-infra". 
After initializing a layer (new_layer = torch.nn.Linear(...)), we can access its weights through the .weight attribute, new_layer.weight. 
Additionally, for convenience, PyTorch allows direct access to all a modelâ€™s trainable parameters, including weights and biases, through 
the method model.parameters(), which we will use later when implementing the model training.

pip show torch
Name: torch
Version: 2.2.2
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: packages@pytorch.org
License: BSD-3
Location: /opt/anaconda3/lib/python3.11/site-packages
Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions
Required-by: accelerate, bertviz, deepspeed, peft



