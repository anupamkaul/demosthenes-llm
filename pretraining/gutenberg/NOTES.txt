Here's a guide to preparing 60,000 public domain books from Project Gutenberg for LLM training:

1. Bulk download and extract the data

Download Options:

Project Gutenberg offers a bulk download option for the entire collection of English books in a 
compressed ZIM file (around 40 GB). You can open it with Kiwix.

You can also use tools like wget with a spider mode to download plain text English files at an acceptable 
rate. This might take 36 to 48 hours for a fast connection, according to Ex Ratione.

Another option is using the bulk downloader tool at http://pgiso.pglaf.org/ which allows specifying ID ranges, 
file types, and languages for download.

File Format: Aim for plain text (.txt) files, as they are easier to process for LLM training compared to formats like PDF.

Extraction: If you download compressed archives (like .zip or .gz), unzip them and organize the extracted text files into a 
single directory for easier processing. 

2. Preliminary text cleaning

Remove Project Gutenberg Headers and Footers: Each Project Gutenberg book includes a header and footer that should be removed. 
Look for markers like "*** START OF THIS PROJECT GUTENBERG EBOOK [TITLE] " and " END OF THIS PROJECT GUTENBERG EBOOK ***" to 
identify and strip these sections. Be mindful of variations in capitalization.

Unicode Fixing and Language Identification: Handle potential encoding errors or non-standard characters in the text, converting them 
to UTF-8 or replacing problematic characters. For multilingual datasets, identify and separate the languages to create a cohesive corpus.

Remove Unnecessary Elements: Remove elements that might not be relevant for training, like HTML tags, scripts, navigation menus, headers, 
and footers (if present in the text files). Libraries like BeautifulSoup and frameworks like LangChain can help with this.

Normalize Text:

Convert HTML entities (e.g.,  ) to their character equivalents.
Convert text to lowercase, unless case sensitivity is crucial for the LLM.
Standardize fancy quotation marks and punctuation.
Remove or replace problematic non-UTF-8 characters. 

3. Heuristic and model-based filtering

Heuristic Filtering:

Remove Low-Quality Texts: Identify and eliminate spam, cryptic fragments, or trivially short texts.

Filter Based on Quality Criteria: You might implement custom filters, for example, to exclude books with 
too many short sentences or specific patterns of words, which could indicate lower quality text for LLM training.

Filter Based on Author Age: For certain LLM applications, you may want to focus on authors from a specific time period 
to ensure the language resembles current usage. For example, filtering for authors who lived until the 20th century 
could be a relevant heuristic.

Model-Based Filtering:

Quality Filtering: Employ models to evaluate and filter content based on specific quality metrics, such as coherence, 
fluency, and relevance.

PII Redaction: Identify and remove personally identifiable information (PII) to ensure privacy and compliance.

Toxicity Filtering: Remove toxic language, including hate speech and biased content, to prevent the LLM from generating 
offensive responses.

Deduplication:
Exact Deduplication: Remove exact duplicates of documents within the dataset.
Fuzzy and Semantic Deduplication: Utilize techniques like MinHash and LSH to identify and remove near-duplicate documents 
or semantically similar documents, which helps improve model training efficiency and reduce computational costs. 

4. Tokenization and chunk splitting

Tokenization:
Choose a tokenizer that is appropriate for your chosen LLM architecture.
Tokenization is crucial for converting text into numerical values that the computer can process.

Chunk Splitting:
Large documents should be split into smaller, manageable chunks that can be processed by the LLM during training.
Consider different chunking strategies depending on your LLM and task requirements. 

5. Ethical considerations and data governance

Copyright and Licensing: Ensure that you have the right to use the Project Gutenberg books for LLM training and 
adhere to Project Gutenberg's policies.

Privacy: If the data contains any sensitive information, apply anonymization techniques to protect individuals' 
privacy and comply with regulations like GDPR or CCPA.

Bias: Be mindful of potential biases in the historical texts and consider how they might impact the LLM's outputs. 
You can actively inspect the datasets for biases and use bias detection tools to identify and address them. 

6. Pretraining and evaluation

Pretraining:

Use your chosen LLM and training code (e.g., Axolotl) to pretrain the model on the prepared dataset.

Monitor the training process and evaluate the model's performance on appropriate benchmarks.

Evaluate Dataset Quality: Regularly assess the quality of your dataset to ensure it's contributing effectively to the LLM's performance. 

By following these steps, you can effectively prepare a large dataset of Project Gutenberg books for training your LLM, leading to a more 
robust and accurate model. 

