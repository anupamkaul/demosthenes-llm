python pretraining_simple.py                      
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=1, print_sample_iter=1000, eval_freq=100, save_ckpt_freq=100000, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  0 

Tokenizing file 1 of 36: data/preprocessed.0/combined_9.txt
Traceback (most recent call last):
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 260, in <module>
    train_losses, val_losses, tokens_seen = train_model_simple(
                                            ^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 113, in train_model_simple
    train_loader, val_loader = create_dataloaders(
                               ^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 44, in create_dataloaders
    train_loader = create_dataloader_v1(
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/../../../tokenizers/dataloaderV1.py", line 17, in create_dataloader_v1
    dataset = GPTDatasetV1.GPTDatasetV1(txt, tokenizer, max_length, stride)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/../../../tokenizers/GPTDatasetV1.py", line 29, in __init__
    token_ids = tokenizer.encode(txt)  
                ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/tiktoken/core.py", line 117, in encode
    raise_disallowed_special_token(match.group())
  File "/opt/anaconda3/lib/python3.11/site-packages/tiktoken/core.py", line 400, in raise_disallowed_special_token
    raise ValueError(
ValueError: Encountered text corresponding to disallowed special token '<|endoftext|>'.
If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.
If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.
To disable this check for all special tokens, pass `disallowed_special=()`.

--> FIXED, next error:

python pretraining_simple.py 
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=1, print_sample_iter=1000, eval_freq=100, save_ckpt_freq=100000, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  0 


Splitting file 1 of 36: data/preprocessed.0/combined_9.txt into a 0.9 split between train and validation

and Tokenizing file 1 of 36: data/preprocessed.0/combined_9.txt
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
Training ...
Traceback (most recent call last):
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 267, in <module>
    train_losses, val_losses, tokens_seen = train_model_simple(
                                            ^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 132, in train_model_simple
    loss = calc_loss_batch(input_batch, target_batch, model, device)
           ^^^^^^^^^^^^^^^
NameError: name 'calc_loss_batch' is not defined

--> 

FIXED, next:

python pretraining_simple.py 
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=1, print_sample_iter=1000, eval_freq=100, save_ckpt_freq=100000, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  0 


Splitting file 1 of 36: data/preprocessed.0/combined_9.txt into a 0.9 split between train and validation

and Tokenizing file 1 of 36: data/preprocessed.0/combined_9.txt
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
Training ...
Ep 1 (Step 0): Train loss 7.914, Val loss 10.214
Traceback (most recent call last):
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 269, in <module>
    train_losses, val_losses, tokens_seen = train_model_simple(
                                            ^^^^^^^^^^^^^^^^^^^
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 152, in train_model_simple
    generate_and_print_sample(
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/../../../pretraining/utils_loss.py", line 77, in generate_and_print_sample
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
              ^^^^^^^^^^^^^^^^^
NameError: name 'text_to_token_ids' is not defined

--> FIXED and Training now works, interrupted and see model is saved. One error with plot_losses

python pretraining_simple.py 
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=1, print_sample_iter=1000, eval_freq=100, save_ckpt_freq=100000, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  0 


Splitting file 1 of 36: data/preprocessed.0/combined_9.txt into a 0.9 split between train and validation

and Tokenizing file 1 of 36: data/preprocessed.0/combined_9.txt
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
Training ...
Ep 1 (Step 0): Train loss 7.914, Val loss 10.214
Every effort moves you/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8/8
^C
Saved model_checkpoints/model_pg_0_interrupted.pth
Saved model_checkpoints/model_pg_final.pth
Traceback (most recent call last):
  File "/Users/anupkaul/akaul_git/demosthenes-llm/pretraining/gutenberg/gutenberg/pretraining_simple.py", line 283, in <module>
    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, output_dir)
TypeError: plot_losses() takes 4 positional arguments but 5 were given

--> Some BUGS
: showing with saved variables that when global counter is 0 the loop will quit (max_eval_limit)
: if loop is interrupted (Ctl-C) during tokenization of the previous file then previous loop's batch counter is saved (needs to be fixed to zero)

(base) anupkaul@147dda4c0851 gutenberg % python pretraining_simple.py 
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=10, print_sample_iter=5, eval_freq=5, save_ckpt_freq=10, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
loaded previously saved model to continue training..
loading training state:  {'n_epochs': 5, 'file_enum': 20, 'input_batch_counter': 0, 'tokens_seen': 3, 'global_step': 4}
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  5  of  10 

batch size  4
new index:  20 file path:  data/preprocessed.0/combined_14.txt <ENTER>
Reading and splitting file 20 of 36: data/preprocessed.0/combined_14.txt into a 0.9 split between train and validation
Tokenizing file 20 of 36: data/preprocessed.0/combined_14.txt
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!

Training ...
1  input batch:  tensor([[  447,   247,    82,  ...,   286,  2677, 13514],
        [  198,   198, 27034,  ...,    40,  4601,   326],
        [25813,    13, 14423,  ...,   220,   220,   220],
        [  250,  6690,   349,  ...,  2256,    11,   447]]) 
target batch :  tensor([[  247,    82,   198,  ...,  2677, 13514,    13],
        [  198, 27034,    13,  ...,  4601,   326,   673],
        [   13, 14423,   717,  ...,   220,   220,   220],
        [ 6690,   349, 26448,  ...,    11,   447,   251]])
global step:  0  tokens seen:  4096
evaluating model, noting train + validation loss (interim)
Ep 6 (Step 0): Train loss 7.630, Val loss 7.597

generate and print sample..
Every effort moves you                                                  
reached max eval limit, moving to next book


generate and print sample..
Every effort moves you                                                  

out of input_batch inner for loop

some stats: 
Book processed 0h 4m 52s
Total time elapsed 0h 4m 52s
ETA for remaining books: 0h 3m 54s
new index:  21 file path:  data/preprocessed.0/combined_28.txt <ENTER>
Reading and splitting file 21 of 36: data/preprocessed.0/combined_28.txt into a 0.9 split between train and validation
Tokenizing file 21 of 36: data/preprocessed.0/combined_28.txt
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!
generated token IDs !
now chunking tokens for max_length =  1024  stride =  1024
tokenization done!

Training ...
1  input batch:  tensor([[ 4548, 17809, 24685,  ...,    82,   551,  8591],
        [12421,   198,  2385,  ...,  5173,   408, 11033],
        [   11,  6273,   415,  ...,    83,   220, 25125],
        [  361,   417,   568,  ...,  1931,  4169,  1611]]) 
target batch :  tensor([[17809, 24685, 16175,  ...,   551,  8591, 13808],
        [  198,  2385,    71,  ...,   408, 11033, 11033],
        [ 6273,   415,   257,  ...,   220, 25125,  2634],
        [  417,   568,    71,  ...,  4169,  1611,   677]])
global step:  1  tokens seen:  8192
2  input batch:  tensor([[  257,   385,  4587,  ...,  6732, 11033,  2261],
        [ 8073,   287,   644,  ...,   607,  1282,   287],
        [ 3656,    11,   447,  ...,    11,   290,   198],
        [  198,   379,   262,  ...,   338, 11114,   290]]) 
target batch :  tensor([[  385,  4587,   198,  ..., 11033,  2261,   259],
        [  287,   644,   550,  ...,  1282,   287,    11],
        [   11,   447,   251,  ...,   290,   198, 15344],
        [  379,   262,  2526,  ..., 11114,   290,  2842]])
global step:  2  tokens seen:  12288
3  input batch:  tensor([[10034, 12409,  2489,  ..., 12931,   422,   438],
        [  290, 13002,   508,  ...,   198,   198,     1],
        [ 8358,   269,     6,  ..., 12797,  8358,   198],
        [ 2584,    76, 11033,  ...,    13, 22673,   289]]) 
target batch :  tensor([[12409,  2489,    11,  ...,   422,   438,   259],
        [13002,   508, 10408,  ...,   198,     1, 16676],
        [  269,     6,   395,  ...,  8358,   198,  6880],
        [   76, 11033,    83,  ..., 22673,   289, 11033]])
global step:  3  tokens seen:  16384
4  input batch:  tensor([[  306,    11,  1165,  ...,   379,   257,  3084],
        [  257,  3105,  2046,  ..., 42200,  8073, 11496],
        [ 4928,    11,   340,  ...,  5822,   198, 32826],
        [  262,  1597,   286,  ..., 20374,    26,   340]]) 
target batch :  tensor([[   11,  1165,    11,  ...,   257,  3084,   655],
        [ 3105,  2046,   287,  ...,  8073, 11496,   198],
        [   11,   340,   373,  ...,   198, 32826,   340],
        [ 1597,   286,  1204,  ...,    26,   340,  2058]])
global step:  4  tokens seen:  20480
5  input batch:  tensor([[  447,   247,    82,  ...,   611,   655, 49975],
        [  477,   268,  6184,  ...,  3693, 23516,    60],
        [ 6058,  3754,   198,  ...,   500,   373, 10032],
        [  198,  6214,   264,  ...,  2185,    11,   921]]) 
target batch :  tensor([[  247,    82,   644,  ...,   655, 49975,    26],
        [  268,  6184,   120,  ..., 23516,    60,   198],
        [ 3754,   198,  1659,  ...,   373, 10032,   286],
        [ 6214,   264,    13,  ...,    11,   921,  9365]])
global step:  5  tokens seen:  24576
evaluating model, noting train + validation loss (interim)
Ep 6 (Step 5): Train loss 7.011, Val loss 7.464

generate and print sample..
Every effort moves you                                                  
reached max eval limit, moving to next book


generate and print sample..
Every effort moves you                                                  

out of input_batch inner for loop

saving model (interim)
Saved model_checkpoints/model_pg_5.pth
Saved model_checkpoints/model_pg_final.pth
some stats: 
Book processed 0h 9m 2s
Total time elapsed 0h 13m 55s
ETA for remaining books: 0h 9m 56s
new index:  22 file path:  data/preprocessed.0/combined_29.txt <ENTER>
Reading and splitting file 22 of 36: data/preprocessed.0/combined_29.txt into a 0.9 split between train and validation
Tokenizing file 22 of 36: data/preprocessed.0/combined_29.txt
^C
Saved model_checkpoints/model_pg_5_interrupted.pth
Saved model_checkpoints/model_pg_final.pth
Training state saved for epoch 5 file index 22 batch_counter  5
Saved training state
Maximum GPU memory allocated: 0.00 GB
(base) anupkaul@147dda4c0851 gutenberg % 

--> LOG on Aug 15 5:06pm (2025)

(base) anupkaul@147dda4c0851 gutenberg % python pretraining_simple.py 
pretraining args:  Namespace(data_dir='data/preprocessed.0', output_dir='model_checkpoints', n_epochs=10, print_sample_iter=5, eval_freq=5, save_ckpt_freq=10, lr=0.0005, batch_size=4, debug=False)
device for training:  cpu
is MPS available:  False
loaded previously saved model to continue training..
loading training state:  {'n_epochs': 0, 'file_enum': 2, 'input_batch_counter': 2, 'tokens_seen': 16384, 'global_step': 4}
Total files for training: 36
Files:
 ['data/preprocessed.0/combined_9.txt', 'data/preprocessed.0/combined_21.txt', 'data/preprocessed.0/combined_35.txt', 'data/preprocessed.0/combined_34.txt', 'data/preprocessed.0/combined_20.txt', 'data/preprocessed.0/combined_8.txt', 'data/preprocessed.0/combined_36.txt', 'data/preprocessed.0/combined_22.txt', 'data/preprocessed.0/combined_23.txt', 'data/preprocessed.0/combined_33.txt', 'data/preprocessed.0/combined_27.txt', 'data/preprocessed.0/combined_26.txt', 'data/preprocessed.0/combined_32.txt', 'data/preprocessed.0/combined_18.txt', 'data/preprocessed.0/combined_24.txt', 'data/preprocessed.0/combined_30.txt', 'data/preprocessed.0/combined_31.txt', 'data/preprocessed.0/combined_25.txt', 'data/preprocessed.0/combined_19.txt', 'data/preprocessed.0/combined_14.txt', 'data/preprocessed.0/combined_28.txt', 'data/preprocessed.0/combined_29.txt', 'data/preprocessed.0/combined_15.txt', 'data/preprocessed.0/combined_1.txt', 'data/preprocessed.0/combined_3.txt', 'data/preprocessed.0/combined_17.txt', 'data/preprocessed.0/combined_16.txt', 'data/preprocessed.0/combined_2.txt', 'data/preprocessed.0/combined_6.txt', 'data/preprocessed.0/combined_12.txt', 'data/preprocessed.0/combined_13.txt', 'data/preprocessed.0/combined_7.txt', 'data/preprocessed.0/combined_5.txt', 'data/preprocessed.0/combined_11.txt', 'data/preprocessed.0/combined_10.txt', 'data/preprocessed.0/combined_4.txt']

training for epoch  0  of  10 

batch size  4
new index:  2 file path:  data/preprocessed.0/combined_21.txt <ENTER>
Reading and splitting file 2 of 36: data/preprocessed.0/combined_21.txt into a 0.9 split between train and validation
Tokenizing file 2 of 36: data/preprocessed.0/combined_21.txt
GPTDatasetV1.py generated token IDs !
GPTDatasetV1.py chunking tokens for max_length =  1024  stride =  1024
GPTDatasetV1.py tokenization done!
GPTDatasetV1.py generated token IDs !
GPTDatasetV1.py chunking tokens for max_length =  1024  stride =  1024
GPTDatasetV1.py tokenization done!

Training ...
2  input batch:  tensor([[  220,  3336, 25703,  ..., 11778,  5223,   523],
        [  262, 17288,    12,  ..., 10678,   326,   883],
        [ 4760,    51, 19555,  ...,  4760,    34, 10659],
        [   11,   373,   262,  ...,    11,   314,   561]]) 
target batch :  tensor([[ 3336, 25703,  1961,  ...,  5223,   523,   329],
        [17288,    12,  2256,  ...,   326,   883,   508],
        [   51, 19555, 15916,  ...,    34, 10659, 15751],
        [  373,   262,   198,  ...,   314,   561,   407]])
global step:  5  tokens seen:  20480
evaluating model, noting train + validation loss (interim)
Ep 1 (Step 5): Train loss 7.719, Val loss 8.493

generate and print sample..
Every effort moves you                                                  
3  input batch:  tensor([[   32, 17139,    38,  ..., 29697,  1404,  4177],
        [   34,  4760,    34,  ...,  4093,  2246,  4177],
        [   38,  4760,  5603,  ...,   198,  4760,    38],
        [ 2085,   268,  1976,  ...,  2304,   831,  4587]]) 
target batch :  tensor([[17139,    38, 15751,  ...,  1404,  4177,    34],
        [ 4760,    34,  3838,  ...,  2246,  4177,    38],
        [ 4760,  5603,  1404,  ...,  4760,    38,  3838],
        [  268,  1976,    84,  ...,   831,  4587, 35893]])
global step:  6  tokens seen:  24576
4  input batch:  tensor([[  290, 46728,   863,  ...,  3843, 11015,  3963],
        [11190, 15916, 11190,  ...,  6144,  6144,  6144],
        [15751,    51, 11190,  ...,  3838,  4760, 42197],
        [   13,   198,   198,  ...,   531,   340,  1625]]) 
target batch :  tensor([[46728,   863,   416,  ..., 11015,  3963,   350],
        [15916, 11190, 26861,  ...,  6144,  6144,  6144],
        [   51, 11190,    51,  ...,  4760, 42197,    51],
        [  198,   198,   447,  ...,   340,  1625,   422]])
global step:  7  tokens seen:  28672
5  input batch:  tensor([[9476,  287, 4145,  ...,  257,  308, 1000],
        [ 274, 2634,   70,  ...,  317,  479,  494],
        [  11,  198, 6814,  ...,   11,  288, 1236],
        [6144, 6144, 6144,  ..., 6144, 6144, 6144]]) 
target batch :  tensor([[  287,  4145, 41797,  ...,   308,  1000,   355],
        [ 2634,    70,  9116,  ...,   479,   494,  1360],
        [  198,  6814, 39683,  ...,   288,  1236,   198],
        [ 6144,  6144,  6144,  ...,  6144,  6144,  6144]])
global step:  8  tokens seen:  32768
6  input batch:  tensor([[ 4177,    51,  4177,  ..., 19555,  4093,  4760],
        [ 8358,   299,   516,  ...,    26, 12777,   285],
        [ 4587,  6478,  6607,  ..., 21638,   585, 34984],
        [ 6144,  6144,  6144,  ...,  6144,  6144,  6144]]) 
target batch :  tensor([[   51,  4177, 11190,  ...,  4093,  4760, 15916],
        [  299,   516,  1196,  ..., 12777,   285,     6],
        [ 6478,  6607,    11,  ...,   585, 34984,  5719],
        [ 6144,  6144,  6144,  ...,  6144,  6144,  6144]])
global step:  9  tokens seen:  36864
7  input batch:  tensor([[  819,   499,  6944,  ...,    11,  1276,    11],
        [ 3838, 26861,  2246,  ...,  3838,  2246, 46361],
        [ 2246,  4760,  4825,  ...,  1404,  8141,  1404],
        [19555,  4177,  4177,  ..., 19555, 29697,  1404]]) 
target batch :  tensor([[  499,  6944,   290,  ...,  1276,    11,   287],
        [26861,  2246,  1404,  ...,  2246, 46361, 19555],
        [ 4760,  4825,  1404,  ...,  8141,  1404,    38],
        [ 4177,  4177,    51,  ..., 29697,  1404,    38]])
global step:  10  tokens seen:  40960
evaluating model, noting train + validation loss (interim)
Ep 1 (Step 10): Train loss 7.263, Val loss 7.928

generate and print sample..
Every effort moves you                                                  
8  input batch:  tensor([[  795,  2865,    64,  ..., 10134,   256, 28749],
        [  257,  4017, 23151,  ...,   306,   780,   326],
        [10659,    38, 17922,  ...,    38, 46361,  4760],
        [  198,  1350, 29811,  ..., 13951,  5899,   290]]) 
target batch :  tensor([[ 2865,    64,   257,  ...,   256, 28749,  6937],
        [ 4017, 23151,  2063,  ...,   780,   326,   373],
        [   38, 17922,    38,  ..., 46361,  4760,    38],
        [ 1350, 29811,   326,  ...,  5899,   290,  2829]])
^CSaved model_checkpoints/model_pg_10_interrupted.pth
Saved model_checkpoints/model_pg_final.pth
Training state saved for epoch 0 file index 2 batch_counter  8 tokens 40960 global_step 10
Saved training state
tokens_seen[] =  [20480, 40960]
train_losses[] =  [7.718893527984619, 7.262941360473633]
val_losses[] =  [8.492859840393066, 7.928170204162598]
Maximum GPU memory allocated: 0.00 GB

